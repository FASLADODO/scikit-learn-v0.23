{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Cross Validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics))\n",
    "- Learning & testing a model on the same data is a mistake. The model will simply repeat the labels it has already seen, but fail to predict anything new. (This is \"overfitting\").\n",
    "- It is a common practice to reserve a subset of data (a \"test set\") to avoid this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((150, 4), (150,))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "\n",
    "X, y = datasets.load_iris(return_X_y=True)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- now sample the training data while holding 40% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 4) (90,) (60, 4) (60,)\n",
      "0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.4, \n",
    "    random_state=0)\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "clf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)\n",
    "print(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There is still a risk of overfitting on test data, because params can be tweaked until the model works as planned - this means test data \"knowledge\" can leak into the model.\n",
    "- Reserving yet another subset for *validation* solves this problem, but introduces another - we are reducing the #samples available for training.\n",
    "- CV solves this problem. We still need a test data subset; the validation is done instead by splitting the training data into _k_ smaller sets. Each \"fold\" is trained using the remaining k-1 folds as training data.\n",
    "- The performance returned by k-fold cross validation is the average of the values found by the loop.\n",
    "\n",
    "![illustration](px/grid_search_cross_validation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [CV Math](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score) using cross_val_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.96666667, 1.        , 0.96666667, 0.96666667, 1.        ])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "clf = svm.SVC(kernel='linear', C=1)\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.98 (+/- 0.03)\n"
     ]
    }
   ],
   "source": [
    "# mean score & 95% confidence interval of the score estimate\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.96658312, 1.        , 0.96658312, 0.96658312, 1.        ])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# changing the scoring method:\n",
    "from sklearn import metrics\n",
    "scores = cross_val_score(\n",
    "    clf, X, y, cv=5, scoring='f1_macro')\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.97777778, 0.97777778, 1.        , 0.95555556, 1.        ])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# changing the cross-validation iterator method:\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "n_samples = X.shape[0]\n",
    "cv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=0)\n",
    "cross_val_score(clf, X, y, cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.97333333])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using an iterable which yields (train,test) splits as arrays\n",
    "def custom_cv_2folds(X):\n",
    "    n = X.shape[0]\n",
    "    i = 1\n",
    "    while i <= 2:\n",
    "        idx = np.arange(n * (i - 1) / 2, n * i / 2, dtype=int)\n",
    "        yield idx, idx\n",
    "        i += 1\n",
    "\n",
    "custom_cv = custom_cv_2folds(X)\n",
    "cross_val_score(clf, X, y, cv=custom_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [cross_validate](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate) vs cross_val_score:\n",
    "- cross_validate allows using multiple metrics\n",
    "- cross_validate returns a dict with fit-times, score-times & test score.\n",
    "- metrics can be spec'd with a list, tuple or set of scorer names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.96666667, 1.        , 0.96666667, 0.96666667, 1.        ])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "scoring = ['precision_macro', \n",
    "           'recall_macro']\n",
    "\n",
    "clf = svm.SVC(kernel='linear', \n",
    "              C=1, \n",
    "              random_state=0)\n",
    "\n",
    "scores = cross_validate(clf, X, y, scoring=scoring)\n",
    "\n",
    "sorted(scores.keys())\n",
    "scores['test_recall_macro']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- or with a dict that maps a scorer name to predefined or custom function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.975     , 0.975     , 0.99166667, 0.98333333, 0.98333333])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "scoring = {'prec_macro': 'precision_macro',\n",
    "           'rec_macro':  make_scorer(recall_score, \n",
    "                                     average='macro')}\n",
    "\n",
    "scores = cross_validate(clf, X, y, scoring=scoring,\n",
    "                        cv=5, return_train_score=True)\n",
    "\n",
    "sorted(scores.keys())\n",
    "scores['train_rec_macro']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [cross_val_predict](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_predict.html#sklearn.model_selection.cross_val_predict) vs cross_val_score\n",
    "- cross_val_predict returns, for each input element, that element's prediction when it was in the test dataset.\n",
    "- This is usable only in cross-validation strategies that assign all elements to a test set exactly once. Otherwise Scikit raises an exception."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IID (independent, identically distributed) data splits:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [K-fold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold)\n",
    "- divided all samples into k groups of equal sizes.\n",
    "- prediction function is learned using k-1 folds with one left for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 4 5 6 7 8 9] [0 1 2]\n",
      "[0 1 2 6 7 8 9] [3 4 5]\n",
      "[0 1 2 3 4 5 8 9] [6 7]\n",
      "[0 1 2 3 4 5 6 7] [8 9]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "X = [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\"]\n",
    "kf = KFold(n_splits=4)\n",
    "for train, test in kf.split(X):\n",
    "    print(\"%s %s\" % (train, test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Repeated K-fold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RepeatedKFold.html#sklearn.model_selection.RepeatedKFold)\n",
    "- repeats K-fold n times. Can be used when you need to run multiples of K-fold with different splits in each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 4 5] [1 2 3]\n",
      "[1 2 3] [0 4 5]\n",
      "[0 1 3] [2 4 5]\n",
      "[2 4 5] [0 1 3]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "X = np.array([[1,2], [3,4], [5,6], [1,2], [3,4], [5,6]])\n",
    "random_state = 12883823\n",
    "rkf = RepeatedKFold(n_splits=2, \n",
    "                    n_repeats=2, \n",
    "                    random_state=random_state)\n",
    "\n",
    "for train, test in rkf.split(X):\n",
    "    print(\"%s %s\" % (train, test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Leave one out](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.LeaveOneOut.html#sklearn.model_selection.LeaveOneOut) (LOO)\n",
    "- Each training set is created by using all samples except one, which is used for testing. For n samples, we n training sets + n test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4 5] [0]\n",
      "[0 2 3 4 5] [1]\n",
      "[0 1 3 4 5] [2]\n",
      "[0 1 2 4 5] [3]\n",
      "[0 1 2 3 5] [4]\n",
      "[0 1 2 3 4] [5]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "X = [1, 2, 3, 4, 5, 6]\n",
    "loo = LeaveOneOut()\n",
    "for train, test in loo.split(X):\n",
    "    print(\"%s %s\" % (train, test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Leave P out](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.LeavePOut.html#sklearn.model_selection.LeavePOut) (LPO)\n",
    "- creates all possible training & test subsets by removing P samples from the complete set. For n samples this generates ${n \\choose p}$ train-test pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3 4 5] [0 1]\n",
      "[1 3 4 5] [0 2]\n",
      "[1 2 4 5] [0 3]\n",
      "[1 2 3 5] [0 4]\n",
      "[1 2 3 4] [0 5]\n",
      "[0 3 4 5] [1 2]\n",
      "[0 2 4 5] [1 3]\n",
      "[0 2 3 5] [1 4]\n",
      "[0 2 3 4] [1 5]\n",
      "[0 1 4 5] [2 3]\n",
      "[0 1 3 5] [2 4]\n",
      "[0 1 3 4] [2 5]\n",
      "[0 1 2 5] [3 4]\n",
      "[0 1 2 4] [3 5]\n",
      "[0 1 2 3] [4 5]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import LeavePOut\n",
    "\n",
    "X = np.ones(6)\n",
    "lpo = LeavePOut(p=2)\n",
    "for train, test in lpo.split(X):\n",
    "    print(\"%s %s\" % (train, test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random permutations, aka [shuffle split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ShuffleSplit.html#sklearn.model_selection.ShuffleSplit)\n",
    "- Generates a user-defined number of independent train & test subsets.\n",
    "- Randomness can be controlled via ```random_state```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17  6 13  4  2  5 14  9  7 16 11  3  0 15 12] [18  1 19  8 10]\n",
      "[12 19 16 10  0  3  4 15  8 13  9  5 14  7  6] [11  1 18 17  2]\n",
      "[ 2  8  6  3 17  4 10 16 18  9  1  0  7 14 19] [15 13 12  5 11]\n",
      "[17  7 12 14 16 11 10  9 15  1 19  8  6  5  4] [18  0 13  2  3]\n",
      "[18  8 17 15 16  6 13 11  4 10  9 12  3 14  0] [ 7  1  2 19  5]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "X = np.arange(20)\n",
    "ss = ShuffleSplit(n_splits=5, test_size=0.25, random_state=0)\n",
    "for train_index, test_index in ss.split(X):\n",
    "    print(\"%s %s\" % (train_index, test_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## class label splits\n",
    "- some classification problems suffer from large imbalances of class distributions. stratified sampling helps ensure that relative class frequencies are preserved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [stratified K-fold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html#sklearn.model_selection.StratifiedKFold)\n",
    "- each fold contains approx the same percentage of samples of each class as the complete set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train -  [30  3]   |   test -  [15  2]\n",
      "train -  [30  3]   |   test -  [15  2]\n",
      "train -  [30  4]   |   test -  [15  1]\n",
      "train -  [28  5]   |   test -  [17]\n",
      "train -  [28  5]   |   test -  [17]\n",
      "train -  [34]   |   test -  [11  5]\n"
     ]
    }
   ],
   "source": [
    "# compare: stratified 3-fold CV to std k-fold CV.\n",
    "# dataset has 50 samples from 2 unbalanced classes\n",
    "# show #samples in each class\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "import numpy as np\n",
    "\n",
    "X, y = np.ones((50, 1)), np.hstack(([0] * 45, [1] * 5))\n",
    "\n",
    "skf = StratifiedKFold(n_splits=3)\n",
    "kf = KFold(n_splits=3)\n",
    "\n",
    "for train, test in skf.split(X, y):\n",
    "    print('train -  {}   |   test -  {}'.format(\n",
    "        np.bincount(y[train]), np.bincount(y[test])))\n",
    "\n",
    "for train, test in kf.split(X, y):\n",
    "    print('train -  {}   |   test -  {}'.format(\n",
    "        np.bincount(y[train]), np.bincount(y[test])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [stratified shuffle split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html#sklearn.model_selection.StratifiedShuffleSplit)\n",
    "- creates splits by preserving the same percentage for each class as in the complete set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StratifiedShuffleSplit(n_splits=5, random_state=0, test_size=0.5,\n",
      "            train_size=None)\n",
      "TRAIN: [5 2 3] TEST: [4 1 0]\n",
      "TRAIN: [5 1 4] TEST: [0 2 3]\n",
      "TRAIN: [5 0 2] TEST: [4 3 1]\n",
      "TRAIN: [4 1 0] TEST: [2 3 5]\n",
      "TRAIN: [0 5 1] TEST: [3 4 2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])\n",
    "y = np.array([0, 0, 0, 1, 1, 1])\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits = 5, \n",
    "                             test_size = 0.5, \n",
    "                             random_state = 0)\n",
    "sss.get_n_splits(X, y)\n",
    "print(sss)\n",
    "\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## grouped splits\n",
    "- IID assumption breaks if the underlying distribution yields groups of dependent samples.\n",
    "- These grouping are domain specific, for example multiple medical samples taken from each of multiple patients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Group K-Fold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GroupKFold.html#sklearn.model_selection.GroupKFold)\n",
    "- variation of k-fold; ensures the same group is not represented in both the test and training subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5] [6 7 8 9]\n",
      "[0 1 2 6 7 8 9] [3 4 5]\n",
      "[3 4 5 6 7 8 9] [0 1 2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "X      = [0.1, 0.2, 2.2, 2.4, 2.3, 4.55, 5.8, 8.8, 9,   10]\n",
    "y      = [\"a\", \"b\", \"b\", \"b\", \"c\", \"c\",  \"c\", \"d\", \"d\", \"d\"]\n",
    "groups = [ 1,   1,   1,   2,   2,   2,    3,   3,   3,   3]\n",
    "gkf = GroupKFold(n_splits=3)\n",
    "\n",
    "for train, test in gkf.split(X, y, groups=groups):\n",
    "    print(\"%s %s\" % (train, test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Leave one group out](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.LeaveOneGroupOut.html#sklearn.model_selection.LeaveOneGroupOut)\n",
    "- reserves samples according to a 3rd-party array of integer groups. this allows you to encode arbitrary domain information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3 4 5 6] [0 1]\n",
      "[0 1 4 5 6] [2 3]\n",
      "[0 1 2 3] [4 5 6]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "\n",
    "X      = [1, 5, 10, 50, 60, 70, 80]\n",
    "y      = [0, 1, 1,  2,  2,  2,  2]\n",
    "groups = [1, 1, 2,  2,  3,  3,  3]\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "for train, test in logo.split(X, y, groups=groups):\n",
    "    print(\"%s %s\" % (train, test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Leave p groups out](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.LeavePGroupsOut.html#sklearn.model_selection.LeavePGroupsOut)\n",
    "- removes samples related to P groups for each training & test subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 5] [0 1 2 3]\n",
      "[2 3] [0 1 4 5]\n",
      "[0 1] [2 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import LeavePGroupsOut\n",
    "\n",
    "X      = np.arange(6)\n",
    "y      = [1, 1, 1, 2, 2, 2]\n",
    "groups = [1, 1, 2, 2, 3, 3]\n",
    "lpgo   = LeavePGroupsOut(n_groups = 2)\n",
    "\n",
    "for train, test in lpgo.split(X, y, groups=groups):\n",
    "    print(\"%s %s\" % (train, test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Group shuffle split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GroupShuffleSplit.html#sklearn.model_selection.GroupShuffleSplit)\n",
    "- generates a sequence of random partitions, in which a subset of groups is reserved for each split.\n",
    "- this strategy is useful when the behavior of **LeavePGroupsOut** is desired, but the #groups is too large for reasonable compute time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3] [4 5 6 7]\n",
      "[2 3 6 7] [0 1 4 5]\n",
      "[2 3 4 5] [0 1 6 7]\n",
      "[4 5 6 7] [0 1 2 3]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "X      = [0.1, 0.2, 2.2, 2.4, 2.3, 4.55, 5.8, 0.001]\n",
    "y      = [\"a\", \"b\", \"b\", \"b\", \"c\", \"c\",  \"c\", \"a\"]\n",
    "groups = [ 1,   1,   2,   2,   3,   3,    4,   4]\n",
    "gss    = GroupShuffleSplit(n_splits  = 4, \n",
    "                           test_size = 0.5, \n",
    "                           random_state = 0)\n",
    "\n",
    "for train, test in gss.split(X, y, groups=groups):\n",
    "    print(\"%s %s\" % (train, test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [predefined splits](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.PredefinedSplit.html#sklearn.model_selection.PredefinedSplit)\n",
    "- in some datasets a split already exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PredefinedSplit(test_fold=array([ 0,  1, -1,  1]))\n",
      "TRAIN: [1 2 3] TEST: [0]\n",
      "TRAIN: [0 2] TEST: [1 3]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "\n",
    "X         = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
    "y         = np.array([ 0,      0,      1,      1])\n",
    "test_fold =          [ 0,      1,     -1,      1]\n",
    "ps        = PredefinedSplit(test_fold)\n",
    "ps.get_n_splits()\n",
    "\n",
    "print(ps)\n",
    "\n",
    "for train_index, test_index in ps.split():\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [time series splits](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html#sklearn.model_selection.TimeSeriesSplit)\n",
    "- variation of *k-fold* - the first _k_ folds are the training subset; _k+1_ fold is the test subset.\n",
    "- Note that successive training sets are _supersets_ of those that came prior. It adds all surplus data to the first training partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeSeriesSplit(max_train_size=None, n_splits=3)\n",
      "[0 1 2] [3]\n",
      "[0 1 2 3] [4]\n",
      "[0 1 2 3 4] [5]\n"
     ]
    }
   ],
   "source": [
    "# 3-split time series cross-validation, 6-sample dataset\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "X    = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])\n",
    "y    = np.array([1, 2, 3, 4, 5, 6])\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "print(tscv)\n",
    "\n",
    "for train, test in tscv.split(X):\n",
    "    print(\"%s %s\" % (train, test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffling notes\n",
    "- some splitters such as _kfold_ have an inbuilt option to shuffle data before splitting.\n",
    "- this consumes less memory than direct shuffling.\n",
    "- default: no shuffling occurs.\n",
    "- ```random_state=None``` by default - shuffling will be different each time. set ```random_state``` to an integer to get repeated results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
