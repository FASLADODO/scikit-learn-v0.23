{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Feature Extraction - Text](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer)\n",
    "- Scikit provides text-to-numerical feature extraction utilities to aid *tokenization*, *counting* and *normalizing* functions.\n",
    "- A document corpus is defined as a matrix with one row per document and one column per token (word) that occurs in the corpus.\n",
    "- Most corpus examples use small subsets of the total words available, so the resulting matrix will be very sparse. Scikit uses ```scipy.sparse``` to aid computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x9 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 19 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CountVectorizer handles tokenization & occurrence counting.\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "# tokenize & count word occurrences\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# default configuration: extract words of at least 2 letters.\n",
    "analyze = vectorizer.build_analyzer()\n",
    "analyze(\"This is a text document to analyze.\") == (\n",
    "    ['this', 'is', 'text', 'document', 'to', 'analyze'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 0, 1, 0, 1],\n",
       "       [0, 1, 0, 1, 0, 2, 1, 0, 1],\n",
       "       [1, 0, 0, 0, 1, 0, 1, 1, 0],\n",
       "       [0, 1, 1, 1, 0, 0, 1, 0, 1]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names() == (\n",
    "    ['and', 'document', 'first', 'is', 'one',\n",
    "     'second', 'the', 'third', 'this'])\n",
    "\n",
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feature name : column index map is stored in vocabulary_\n",
    "vectorizer.vocabulary_.get('document')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# words not seen in training corpus will be ignored.\n",
    "vectorizer.transform(['Something completely new.']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# previous corpus: 1st & last docs = same #words, \n",
    "# so encoded into equal vectors.\n",
    "# we lose track of last doc being a question.\n",
    "# to preserve some local ordering info, we can use 2-grams.\n",
    "\n",
    "bigram_vectorizer = CountVectorizer(ngram_range=(1, 2),\n",
    "                                    token_pattern=r'\\b\\w+\\b', min_df=1)\n",
    "analyze = bigram_vectorizer.build_analyzer()\n",
    "analyze('Bi-grams are cool!') == (\n",
    "    ['bi', 'grams', 'are', 'cool', 'bi grams', 'grams are', 'are cool'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0],\n",
       "       [0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0],\n",
       "       [0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this vectorizer has a much bigger array.\n",
    "X_2 = bigram_vectorizer.fit_transform(corpus).toarray()\n",
    "X_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we can check for the question phrase..\n",
    "feature_index = bigram_vectorizer.vocabulary_.get('is this')\n",
    "X_2[:, feature_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop Words\n",
    "- Presumably uninformative & okay to ignore, but not always\n",
    "- scikit-learn std stop words list not ideal. See [NQY18](https://scikit-learn.org/stable/modules/feature_extraction.html#id5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tf-Idf term weighting\n",
    "- defined as $\\text{tf-idf(t,d)}=\\text{tf(t,d)} \\times \\text{idf(t)}$\n",
    "- where $\\text{idf}(t) = \\log{\\frac{1 + n}{1+\\text{df}(t)}} + 1$\n",
    "- n = #documents in corpus\n",
    "- df(t) = #documents in corpus containing term _t_\n",
    "- resulting tf-idf vectors are then Euclidean-normalized: $v_{norm} = \\frac{v}{||v||_2} = \\frac{v}{\\sqrt{v{_1}^2 +\n",
    "v{_2}^2 + \\dots + v{_n}^2}}$\n",
    "\n",
    "- scikit-learn implementation of idf differs slightly from std textbook definition. The \"1\" count is added to the idf instead of the idf denominator when ```smooth_idf=False```.\n",
    "    - textbook: $\\text{idf}(t) = \\log{\\frac{n}{1+\\text{df}(t)}}.$\n",
    "    - scikit:   $\\text{idf}(t) = \\log{\\frac{n}{\\text{df}(t)}} + 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer(smooth_idf=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transformer = TfidfTransformer(smooth_idf=False)\n",
    "transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2)\t0.5732079309279059\n",
      "  (0, 0)\t0.8194099510753754\n",
      "  (1, 0)\t1.0\n",
      "  (2, 0)\t1.0\n",
      "  (3, 0)\t1.0\n",
      "  (4, 1)\t0.8808994832762984\n",
      "  (4, 0)\t0.47330339145578754\n",
      "  (5, 2)\t0.8135516873095774\n",
      "  (5, 0)\t0.5814926070688599\n",
      "[[0.81940995 0.         0.57320793]\n",
      " [1.         0.         0.        ]\n",
      " [1.         0.         0.        ]\n",
      " [1.         0.         0.        ]\n",
      " [0.47330339 0.88089948 0.        ]\n",
      " [0.58149261 0.         0.81355169]]\n"
     ]
    }
   ],
   "source": [
    "# first term present 1005 of the time = not too interesting.\n",
    "# 2nd, 3rd terms less so = probably more interesting.\n",
    "counts = [[3, 0, 1],\n",
    "          [2, 0, 0],\n",
    "          [3, 0, 0],\n",
    "          [4, 0, 0],\n",
    "          [3, 2, 0],\n",
    "          [3, 0, 2]]\n",
    "\n",
    "tfidf = transformer.fit_transform(counts)\n",
    "print(tfidf)\n",
    "print(tfidf.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Tf-idf Vectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer) - combines [Count Vectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer) and [Tf-idf Transformer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer) into a single model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x9 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 19 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- While Tf-Idf normalization is useful, there are use cases where binary markers offer better information. Use the ```binary``` parameter of CountVectorizer for this.\n",
    "- For example, Bernoulli NB models discrete boolean random variables.\n",
    "- Also: very short texts have very noisy tf-idf values - the binary version can be more stable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding text files\n",
    "- Text is characters; files are bytes \n",
    "- Common formats: ASCII, Latin-1, KOI8-R, UTF-8, UTF-16, ...\n",
    "- CountVectorizer uses default ```encoding=\"utf-8``` for encoding.\n",
    "- Below: using ```chardet``` to find encoding of three texts, then using CountVectorizer to vectorize them & print out the learned vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sei': 15, 'mir': 13, 'gegrüßt': 6, 'mein': 12, 'sauerkraut': 14, 'holdselig': 10, 'sind': 16, 'deine': 1, 'gerüche': 7, 'auf': 0, 'flügeln': 4, 'des': 2, 'gesanges': 8, 'herzliebchen': 9, 'trag': 17, 'ich': 11, 'dich': 3, 'fort': 5}\n",
      "{'sei': 15, 'mir': 13, 'gegrüßt': 6, 'mein': 12, 'sauerkraut': 14, 'holdselig': 10, 'sind': 16, 'deine': 1, 'gerüche': 7, 'auf': 0, 'flügeln': 4, 'des': 2, 'gesanges': 8, 'herzliebchen': 9, 'trag': 17, 'ich': 11, 'dich': 3, 'fort': 5}\n",
      "{'sei': 15, 'mir': 13, 'gegrüßt': 6, 'mein': 12, 'sauerkraut': 14, 'holdselig': 10, 'sind': 16, 'deine': 1, 'gerüche': 7, 'auf': 0, 'flügeln': 4, 'des': 2, 'gesanges': 8, 'herzliebchen': 9, 'trag': 17, 'ich': 11, 'dich': 3, 'fort': 5}\n",
      "{'sei': 15, 'mir': 13, 'gegrüßt': 6, 'mein': 12, 'sauerkraut': 14, 'holdselig': 10, 'sind': 16, 'deine': 1, 'gerüche': 7, 'auf': 0, 'flügeln': 4, 'des': 2, 'gesanges': 8, 'herzliebchen': 9, 'trag': 17, 'ich': 11, 'dich': 3, 'fort': 5}\n",
      "{'sei': 15, 'mir': 13, 'gegrüßt': 6, 'mein': 12, 'sauerkraut': 14, 'holdselig': 10, 'sind': 16, 'deine': 1, 'gerüche': 7, 'auf': 0, 'flügeln': 4, 'des': 2, 'gesanges': 8, 'herzliebchen': 9, 'trag': 17, 'ich': 11, 'dich': 3, 'fort': 5}\n",
      "{'sei': 15, 'mir': 13, 'gegrüßt': 6, 'mein': 12, 'sauerkraut': 14, 'holdselig': 10, 'sind': 16, 'deine': 1, 'gerüche': 7, 'auf': 0, 'flügeln': 4, 'des': 2, 'gesanges': 8, 'herzliebchen': 9, 'trag': 17, 'ich': 11, 'dich': 3, 'fort': 5}\n",
      "{'sei': 15, 'mir': 13, 'gegrüßt': 6, 'mein': 12, 'sauerkraut': 14, 'holdselig': 10, 'sind': 16, 'deine': 1, 'gerüche': 7, 'auf': 0, 'flügeln': 4, 'des': 2, 'gesanges': 8, 'herzliebchen': 9, 'trag': 17, 'ich': 11, 'dich': 3, 'fort': 5}\n",
      "{'sei': 15, 'mir': 13, 'gegrüßt': 6, 'mein': 12, 'sauerkraut': 14, 'holdselig': 10, 'sind': 16, 'deine': 1, 'gerüche': 7, 'auf': 0, 'flügeln': 4, 'des': 2, 'gesanges': 8, 'herzliebchen': 9, 'trag': 17, 'ich': 11, 'dich': 3, 'fort': 5}\n",
      "{'sei': 15, 'mir': 13, 'gegrüßt': 6, 'mein': 12, 'sauerkraut': 14, 'holdselig': 10, 'sind': 16, 'deine': 1, 'gerüche': 7, 'auf': 0, 'flügeln': 4, 'des': 2, 'gesanges': 8, 'herzliebchen': 9, 'trag': 17, 'ich': 11, 'dich': 3, 'fort': 5}\n",
      "{'sei': 15, 'mir': 13, 'gegrüßt': 6, 'mein': 12, 'sauerkraut': 14, 'holdselig': 10, 'sind': 16, 'deine': 1, 'gerüche': 7, 'auf': 0, 'flügeln': 4, 'des': 2, 'gesanges': 8, 'herzliebchen': 9, 'trag': 17, 'ich': 11, 'dich': 3, 'fort': 5}\n",
      "{'sei': 15, 'mir': 13, 'gegrüßt': 6, 'mein': 12, 'sauerkraut': 14, 'holdselig': 10, 'sind': 16, 'deine': 1, 'gerüche': 7, 'auf': 0, 'flügeln': 4, 'des': 2, 'gesanges': 8, 'herzliebchen': 9, 'trag': 17, 'ich': 11, 'dich': 3, 'fort': 5}\n",
      "{'sei': 15, 'mir': 13, 'gegrüßt': 6, 'mein': 12, 'sauerkraut': 14, 'holdselig': 10, 'sind': 16, 'deine': 1, 'gerüche': 7, 'auf': 0, 'flügeln': 4, 'des': 2, 'gesanges': 8, 'herzliebchen': 9, 'trag': 17, 'ich': 11, 'dich': 3, 'fort': 5}\n",
      "{'sei': 15, 'mir': 13, 'gegrüßt': 6, 'mein': 12, 'sauerkraut': 14, 'holdselig': 10, 'sind': 16, 'deine': 1, 'gerüche': 7, 'auf': 0, 'flügeln': 4, 'des': 2, 'gesanges': 8, 'herzliebchen': 9, 'trag': 17, 'ich': 11, 'dich': 3, 'fort': 5}\n",
      "{'sei': 15, 'mir': 13, 'gegrüßt': 6, 'mein': 12, 'sauerkraut': 14, 'holdselig': 10, 'sind': 16, 'deine': 1, 'gerüche': 7, 'auf': 0, 'flügeln': 4, 'des': 2, 'gesanges': 8, 'herzliebchen': 9, 'trag': 17, 'ich': 11, 'dich': 3, 'fort': 5}\n",
      "{'sei': 15, 'mir': 13, 'gegrüßt': 6, 'mein': 12, 'sauerkraut': 14, 'holdselig': 10, 'sind': 16, 'deine': 1, 'gerüche': 7, 'auf': 0, 'flügeln': 4, 'des': 2, 'gesanges': 8, 'herzliebchen': 9, 'trag': 17, 'ich': 11, 'dich': 3, 'fort': 5}\n",
      "{'sei': 15, 'mir': 13, 'gegrüßt': 6, 'mein': 12, 'sauerkraut': 14, 'holdselig': 10, 'sind': 16, 'deine': 1, 'gerüche': 7, 'auf': 0, 'flügeln': 4, 'des': 2, 'gesanges': 8, 'herzliebchen': 9, 'trag': 17, 'ich': 11, 'dich': 3, 'fort': 5}\n",
      "{'sei': 15, 'mir': 13, 'gegrüßt': 6, 'mein': 12, 'sauerkraut': 14, 'holdselig': 10, 'sind': 16, 'deine': 1, 'gerüche': 7, 'auf': 0, 'flügeln': 4, 'des': 2, 'gesanges': 8, 'herzliebchen': 9, 'trag': 17, 'ich': 11, 'dich': 3, 'fort': 5}\n",
      "{'sei': 15, 'mir': 13, 'gegrüßt': 6, 'mein': 12, 'sauerkraut': 14, 'holdselig': 10, 'sind': 16, 'deine': 1, 'gerüche': 7, 'auf': 0, 'flügeln': 4, 'des': 2, 'gesanges': 8, 'herzliebchen': 9, 'trag': 17, 'ich': 11, 'dich': 3, 'fort': 5}\n"
     ]
    }
   ],
   "source": [
    "import chardet    # doctest: +SKIP\n",
    "text1 = b\"Sei mir gegr\\xc3\\xbc\\xc3\\x9ft mein Sauerkraut\"\n",
    "text2 = b\"holdselig sind deine Ger\\xfcche\"\n",
    "text3 = b\"\\xff\\xfeA\\x00u\\x00f\\x00 \\x00F\\x00l\\x00\\xfc\\x00g\\x00e\\x00l\\x00n\\x00 \\x00d\\x00e\\x00s\\x00 \\x00G\\x00e\\x00s\\x00a\\x00n\\x00g\\x00e\\x00s\\x00,\\x00 \\x00H\\x00e\\x00r\\x00z\\x00l\\x00i\\x00e\\x00b\\x00c\\x00h\\x00e\\x00n\\x00,\\x00 \\x00t\\x00r\\x00a\\x00g\\x00 \\x00i\\x00c\\x00h\\x00 \\x00d\\x00i\\x00c\\x00h\\x00 \\x00f\\x00o\\x00r\\x00t\\x00\"\n",
    "decoded = [x.decode(chardet.detect(x)['encoding'])\n",
    "           for x in (text1, text2, text3)]        # doctest: +SKIP\n",
    "v = CountVectorizer().fit(decoded).vocabulary_    # doctest: +SKIP\n",
    "for term in v: print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications\n",
    "- Supervised: use with linear models to build document classifiers\n",
    "- Unsupervised: used with clustering models to group similar documents.\n",
    "- Learn main topics using NNMF & LDA.\n",
    "- Examples below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: [Text doc classification - sparse features](https://scikit-learn.org/stable/auto_examples/text/plot_document_classification_20newsgroups.html#sphx-glr-auto-examples-text-plot-document-classification-20newsgroups-py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "from optparse import OptionParser\n",
    "import sys\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, ComplementNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils.extmath import density\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display progress logs on stdout\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s %(levelname)s %(message)s')\n",
    "\n",
    "op = OptionParser()\n",
    "op.add_option(\"--report\",\n",
    "              action=\"store_true\", dest=\"print_report\",\n",
    "              help=\"Print a detailed classification report.\")\n",
    "op.add_option(\"--chi2_select\",\n",
    "              action=\"store\", type=\"int\", dest=\"select_chi2\",\n",
    "              help=\"Select some number of features using a chi-squared test\")\n",
    "op.add_option(\"--confusion_matrix\",\n",
    "              action=\"store_true\", dest=\"print_cm\",\n",
    "              help=\"Print the confusion matrix.\")\n",
    "op.add_option(\"--top10\",\n",
    "              action=\"store_true\", dest=\"print_top10\",\n",
    "              help=\"Print ten most discriminative terms per class\"\n",
    "                   \" for every classifier.\")\n",
    "op.add_option(\"--all_categories\",\n",
    "              action=\"store_true\", dest=\"all_categories\",\n",
    "              help=\"Whether to use all categories or not.\")\n",
    "op.add_option(\"--use_hashing\",\n",
    "              action=\"store_true\",\n",
    "              help=\"Use a hashing vectorizer.\")\n",
    "op.add_option(\"--n_features\",\n",
    "              action=\"store\", type=int, default=2 ** 16,\n",
    "              help=\"n_features when using the hashing vectorizer.\")\n",
    "op.add_option(\"--filtered\",\n",
    "              action=\"store_true\",\n",
    "              help=\"Remove newsgroup information that is easily overfit: \"\n",
    "                   \"headers, signatures, and quoting.\")\n",
    "\n",
    "\n",
    "def is_interactive():\n",
    "    return not hasattr(sys.modules['__main__'], '__file__')\n",
    "\n",
    "# work-around for Jupyter notebook and IPython console\n",
    "argv = [] if is_interactive() else sys.argv[1:]\n",
    "(opts, args) = op.parse_args(argv)\n",
    "if len(args) > 0:\n",
    "    op.error(\"this script takes no arguments.\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 20 newsgroups dataset for categories:\n",
      "['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n"
     ]
    }
   ],
   "source": [
    "if opts.all_categories:\n",
    "    categories = None\n",
    "else:\n",
    "    categories = [\n",
    "        'alt.atheism',\n",
    "        'talk.religion.misc',\n",
    "        'comp.graphics',\n",
    "        'sci.space',\n",
    "    ]\n",
    "\n",
    "if opts.filtered:\n",
    "    remove = ('headers', 'footers', 'quotes')\n",
    "else:\n",
    "    remove = ()\n",
    "\n",
    "print(\"Loading 20 newsgroups dataset for categories:\")\n",
    "print(categories if categories else \"all\")\n",
    "\n",
    "data_train = fetch_20newsgroups(subset='train', \n",
    "                                categories=categories,\n",
    "                                shuffle=True, \n",
    "                                random_state=42,\n",
    "                                remove=remove)\n",
    "\n",
    "data_test = fetch_20newsgroups(subset='test', \n",
    "                               categories=categories,\n",
    "                               shuffle=True, \n",
    "                               random_state=42,\n",
    "                               remove=remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2034 documents - 3.980MB (training set)\n",
      "1353 documents - 2.867MB (test set)\n",
      "4 categories\n"
     ]
    }
   ],
   "source": [
    "target_names = data_train.target_names\n",
    "\n",
    "def size_mb(docs):\n",
    "    return sum(len(s.encode('utf-8')) for s in docs) / 1e6\n",
    "\n",
    "data_train_size_mb = size_mb(data_train.data)\n",
    "data_test_size_mb  = size_mb(data_test.data)\n",
    "\n",
    "print(\"%d documents - %0.3fMB (training set)\" % (\n",
    "    len(data_train.data), data_train_size_mb))\n",
    "print(\"%d documents - %0.3fMB (test set)\" % (\n",
    "    len(data_test.data), data_test_size_mb))\n",
    "print(\"%d categories\" % len(target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features\n",
      "done in 0.593683s at 6.703MB/s\n",
      "n_samples: 2034, n_features: 33809\n"
     ]
    }
   ],
   "source": [
    "y_train, y_test = data_train.target, data_test.target\n",
    "\n",
    "print(\"Extracting features\")\n",
    "t0 = time()\n",
    "if opts.use_hashing:\n",
    "    vectorizer = HashingVectorizer(stop_words     = 'english', \n",
    "                                   alternate_sign = False,\n",
    "                                   n_features     = opts.n_features)\n",
    "    X_train = vectorizer.transform(data_train.data)\n",
    "else:\n",
    "    vectorizer = TfidfVectorizer(sublinear_tf     = True, \n",
    "                                 max_df           = 0.5,\n",
    "                                 stop_words       = 'english')\n",
    "    X_train = vectorizer.fit_transform(data_train.data)\n",
    "    \n",
    "duration = time() - t0\n",
    "print(\"done in %fs at %0.3fMB/s\" % (duration, \n",
    "                                    data_train_size_mb / duration))\n",
    "print(\"n_samples: %d, n_features: %d\" % X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features\n",
      "done in 0.358432s at 8.000MB/s\n",
      "n_samples: 1353, n_features: 33809\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracting features\")\n",
    "t0 = time()\n",
    "X_test = vectorizer.transform(data_test.data)\n",
    "duration = time() - t0\n",
    "print(\"done in %fs at %0.3fMB/s\" % (duration, \n",
    "                                    data_test_size_mb / duration))\n",
    "print(\"n_samples: %d, n_features: %d\" % X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if opts.use_hashing:\n",
    "    feature_names = None\n",
    "else:\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "if opts.select_chi2:\n",
    "    print(\"Extracting %d best features: chi-squared test\" %\n",
    "          opts.select_chi2)\n",
    "    \n",
    "    t0 = time()\n",
    "    ch2 = SelectKBest(chi2, k=opts.select_chi2)\n",
    "    X_train = ch2.fit_transform(X_train, y_train)\n",
    "    X_test = ch2.transform(X_test)\n",
    "    \n",
    "    if feature_names:\n",
    "        feature_names = [feature_names[i] for i\n",
    "                         in ch2.get_support(indices=True)]\n",
    "    print(\"done in %fs\" % (time() - t0))\n",
    "\n",
    "if feature_names:\n",
    "    feature_names = np.asarray(feature_names)\n",
    "\n",
    "def trim(s):\n",
    "    return s if len(s) <= 80 else s[:77] + \"...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Classifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bjpcjp/.local/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:556: UserWarning: \"sag\" solver requires many iterations to fit an intercept with sparse inputs. Either set the solver to \"auto\" or \"sparse_cg\", or set a low \"tol\" and a high \"max_iter\" (especially if inputs are not standardized).\n",
      "  '\"sag\" solver requires many iterations to fit '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron\n",
      "Passive-Aggressive\n",
      "kNN\n",
      "Random forest\n",
      "Elastic-Net penalty\n",
      "NearestCentroid (aka Rocchio classifier)\n",
      "Naive Bayes\n",
      "LinearSVC with L1-based feature selection\n"
     ]
    }
   ],
   "source": [
    "# benchmark 15 classifiers\n",
    "def benchmark(clf):\n",
    "    #print(\"Training: \")\n",
    "    #print(clf)\n",
    "    t0 = time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_time = time() - t0\n",
    "    #print(\"train time: %0.3fs\" % train_time)\n",
    "\n",
    "    t0 = time()\n",
    "    pred = clf.predict(X_test)\n",
    "    test_time = time() - t0\n",
    "    #print(\"test time:  %0.3fs\" % test_time)\n",
    "\n",
    "    score = metrics.accuracy_score(y_test, pred)\n",
    "    #print(\"accuracy:   %0.3f\" % score)\n",
    "\n",
    "    if hasattr(clf, 'coef_'):\n",
    "        #print(\"dimensionality: %d\" % clf.coef_.shape[1])\n",
    "        #print(\"density: %f\" % density(clf.coef_))\n",
    "\n",
    "        if opts.print_top10 and feature_names is not None:\n",
    "            #print(\"top 10 keywords per class:\")\n",
    "            for i, label in enumerate(target_names):\n",
    "                top10 = np.argsort(clf.coef_[i])[-10:]\n",
    "                #print(trim(\"%s: %s\" % (label, \" \".join(feature_names[top10]))))\n",
    "\n",
    "    #if opts.print_report:\n",
    "        #print(\"classification report:\")\n",
    "        #print(metrics.classification_report(y_test, pred, target_names=target_names))\n",
    "    #if opts.print_cm:\n",
    "        #print(\"confusion matrix:\")\n",
    "        #print(metrics.confusion_matrix(y_test, pred))\n",
    "\n",
    "    clf_descr = str(clf).split('(')[0]\n",
    "    return clf_descr, score, train_time, test_time\n",
    "\n",
    "\n",
    "results = []\n",
    "for clf, name in (\n",
    "        (RidgeClassifier(tol=1e-2, \n",
    "                         solver=\"sag\"), \n",
    "         \"Ridge Classifier\"),\n",
    "    \n",
    "        (Perceptron(max_iter=50), \n",
    "         \"Perceptron\"),\n",
    "    \n",
    "        (PassiveAggressiveClassifier(max_iter=50),\n",
    "         \"Passive-Aggressive\"),\n",
    "    \n",
    "        (KNeighborsClassifier(n_neighbors=10), \n",
    "         \"kNN\"),\n",
    "    \n",
    "        (RandomForestClassifier(), \n",
    "         \"Random forest\")):\n",
    "    print(name)\n",
    "    results.append(benchmark(clf))\n",
    "\n",
    "for penalty in [\"l2\", \"l1\"]:\n",
    "    #print(\"%s penalty\" % penalty.upper())\n",
    "    # Train Liblinear model\n",
    "    results.append(benchmark(LinearSVC(penalty=penalty, dual=False,\n",
    "                                       tol=1e-3)))\n",
    "\n",
    "    # Train SGD model\n",
    "    results.append(benchmark(SGDClassifier(alpha=.0001, max_iter=50,\n",
    "                                           penalty=penalty)))\n",
    "\n",
    "# Train SGD with Elastic Net penalty\n",
    "print(\"Elastic-Net penalty\")\n",
    "results.append(benchmark(SGDClassifier(alpha=.0001, max_iter=50,\n",
    "                                       penalty=\"elasticnet\")))\n",
    "\n",
    "# Train NearestCentroid without threshold\n",
    "print(\"NearestCentroid (aka Rocchio classifier)\")\n",
    "results.append(benchmark(NearestCentroid()))\n",
    "\n",
    "# Train sparse Naive Bayes classifiers (3)\n",
    "print(\"Naive Bayes\")\n",
    "results.append(benchmark(MultinomialNB(alpha=.01)))\n",
    "results.append(benchmark(BernoulliNB(alpha=.01)))\n",
    "results.append(benchmark(ComplementNB(alpha=.1)))\n",
    "\n",
    "print(\"LinearSVC with L1-based feature selection\")\n",
    "# smaller C = stronger regularization = more sparsity\n",
    "results.append(benchmark(Pipeline([\n",
    "  ('feature_selection', SelectFromModel(LinearSVC(penalty=\"l1\", \n",
    "                                                  dual=False,\n",
    "                                                  tol=1e-3))),\n",
    "  ('classification', LinearSVC(penalty=\"l2\"))])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plots\n",
    "indices = np.arange(len(results))\n",
    "\n",
    "results = [[x[i] for x in results] for i in range(4)]\n",
    "\n",
    "clf_names, score, training_time, test_time = results\n",
    "training_time = np.array(training_time) / np.max(training_time)\n",
    "test_time = np.array(test_time) / np.max(test_time)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.title(\"Score\")\n",
    "plt.barh(indices, score, .2, label=\"score\", color='navy')\n",
    "plt.barh(indices + .3, training_time, .2, label=\"training time\",\n",
    "         color='c')\n",
    "plt.barh(indices + .6, test_time, .2, label=\"test time\", color='darkorange')\n",
    "plt.yticks(())\n",
    "plt.legend(loc='best')\n",
    "plt.subplots_adjust(left=.25)\n",
    "plt.subplots_adjust(top=.95)\n",
    "plt.subplots_adjust(bottom=.05)\n",
    "\n",
    "for i, c in zip(indices, clf_names):\n",
    "    plt.text(-.3, i, c)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: [Text doc clustering](https://scikit-learn.org/stable/auto_examples/text/plot_document_clustering.html#sphx-glr-auto-examples-text-plot-document-clustering-py)\n",
    "- Clusters documents by topics using bag-of-words.\n",
    "- Two feature extractors demo'd:\n",
    "    - **TfidfVectorizer**: uses in-memory vocabulary (Python dict) to map words to feature indices, find a word frequency matrix, then re-weight the matrix using an inverse doc frequency (IDF) vector.\n",
    "    - **HasingVectorizer**: hashes words to word counts in a fixed dimensional space (collisions are possible), normalizes the counts to have an L2-norm equal to one (projected to a Euclidean unit-ball). Does *not* provide IDF weighting - pipeline the output to a Transformer instance if it is needed.\n",
    "\n",
    "- K-means and it scalable minibatch variant are used for comparison.\n",
    "\n",
    "- LSA (latent semantic analysis can be used to reduce dimensionality.\n",
    "\n",
    "- K-means is sensitive to feature scaling - IDF weighting helps to improve cluster quality.\n",
    "\n",
    "- K-means is optimizing a non-convex function - it is likely to end up in a local optimium. Plan on using multiple runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "import logging\n",
    "from optparse import OptionParser\n",
    "import sys\n",
    "from time import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s %(levelname)s %(message)s')\n",
    "\n",
    "op = OptionParser()\n",
    "op.add_option(\"--lsa\",\n",
    "              dest=\"n_components\", \n",
    "              type=\"int\",\n",
    "              help=\"Preprocess documents with LSA.\")\n",
    "op.add_option(\"--no-minibatch\",\n",
    "              action=\"store_false\", \n",
    "              dest=\"minibatch\", \n",
    "              default=True,\n",
    "              help=\"Use ordinary k-means\")\n",
    "op.add_option(\"--no-idf\",\n",
    "              action=\"store_false\", \n",
    "              dest=\"use_idf\", \n",
    "              default=True,\n",
    "              help=\"Disable IDF feature weighting.\")\n",
    "op.add_option(\"--use-hashing\",\n",
    "              action=\"store_true\", \n",
    "              default=False,\n",
    "              help=\"Use hashing feature vectorizer\")\n",
    "op.add_option(\"--n-features\", \n",
    "              type=int, \n",
    "              default=10000,\n",
    "              help=\"Maximum #features (dimensions) to extract\")\n",
    "op.add_option(\"--verbose\",\n",
    "              action=\"store_true\", \n",
    "              dest=\"verbose\", \n",
    "              default=False,\n",
    "              help=\"Print progress reports inside k-means.\")\n",
    "\n",
    "def is_interactive():\n",
    "    return not hasattr(sys.modules['__main__'], '__file__')\n",
    "\n",
    "argv = [] if is_interactive() else sys.argv[1:]\n",
    "(opts, args) = op.parse_args(argv)\n",
    "if len(args) > 0:\n",
    "    op.error(\"this script takes no arguments.\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3387 documents\n",
      "4 categories\n"
     ]
    }
   ],
   "source": [
    "categories = [\n",
    "    'alt.atheism',\n",
    "    'talk.religion.misc',\n",
    "    'comp.graphics',\n",
    "    'sci.space',\n",
    "]\n",
    "# Uncomment the following to do the analysis on all the categories\n",
    "# categories = None\n",
    "dataset = fetch_20newsgroups(subset='all', \n",
    "                             categories=categories,\n",
    "                             shuffle=True, \n",
    "                             random_state=42)\n",
    "\n",
    "print(\"%d documents\" % len(dataset.data))\n",
    "print(\"%d categories\" % len(dataset.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.825450s\n",
      "n_samples: 3387, n_features: 10000\n"
     ]
    }
   ],
   "source": [
    "labels = dataset.target\n",
    "true_k = np.unique(labels).shape[0]\n",
    "\n",
    "t0 = time()\n",
    "if opts.use_hashing:\n",
    "    if opts.use_idf:\n",
    "        hasher = HashingVectorizer(n_features     = opts.n_features,\n",
    "                                   stop_words     = 'english', \n",
    "                                   alternate_sign = False,\n",
    "                                   norm           = None)\n",
    "        vectorizer = make_pipeline(hasher, \n",
    "                                   TfidfTransformer())\n",
    "    else:\n",
    "        vectorizer = HashingVectorizer(n_features     = opts.n_features,\n",
    "                                       stop_words     = 'english',\n",
    "                                       alternate_sign = False, \n",
    "                                       norm           = 'l2')\n",
    "else:\n",
    "    vectorizer = TfidfVectorizer(max_df       = 0.5, \n",
    "                                 max_features = opts.n_features,\n",
    "                                 min_df       = 2, \n",
    "                                 stop_words   = 'english',\n",
    "                                 use_idf      = opts.use_idf)\n",
    "X = vectorizer.fit_transform(dataset.data)\n",
    "\n",
    "print(\"done in %fs\" % (time() - t0))\n",
    "print(\"n_samples: %d, n_features: %d\" % X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if opts.n_components:\n",
    "    print(\"Performing dimensionality reduction using LSA\")\n",
    "    t0 = time()\n",
    "    # Vectorizer results are normalized, which makes KMeans behave as\n",
    "    # spherical k-means for better results. Since LSA/SVD results are\n",
    "    # not normalized, we have to redo the normalization.\n",
    "    svd = TruncatedSVD(opts.n_components)\n",
    "    normalizer = Normalizer(copy=False)\n",
    "    lsa = make_pipeline(svd, normalizer)\n",
    "\n",
    "    X = lsa.fit_transform(X)\n",
    "\n",
    "    print(\"done in %fs\" % (time() - t0))\n",
    "\n",
    "    explained_variance = svd.explained_variance_ratio_.sum()\n",
    "    print(\"Explained variance of the SVD step: {}%\".format(\n",
    "        int(explained_variance * 100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering sparse data with MiniBatchKMeans(batch_size=1000, init_size=1000, n_clusters=4, n_init=1,\n",
      "                verbose=False)\n",
      "done in 0.149s            \t\n",
      "Homogeneity: 0.414        \t\n",
      "Completeness: 0.435       \t\n",
      "V-measure: 0.424          \t\n",
      "Adjusted Rand-Index: 0.271 \t\n",
      "Silhouette Coefficient: 0.007 \t\n"
     ]
    }
   ],
   "source": [
    "if opts.minibatch:\n",
    "    km = MiniBatchKMeans(n_clusters=true_k, init='k-means++', n_init=1,\n",
    "                         init_size=1000, batch_size=1000, verbose=opts.verbose)\n",
    "else:\n",
    "    km = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1,\n",
    "                verbose=opts.verbose)\n",
    "\n",
    "print(\"Clustering sparse data with %s\" % km)\n",
    "t0 = time()\n",
    "km.fit(X)\n",
    "print(\"done in %0.3fs            \\t\" \n",
    "      % (time() - t0))\n",
    "print(\"Homogeneity: %0.3f        \\t\" \n",
    "      % metrics.homogeneity_score(labels, km.labels_))\n",
    "print(\"Completeness: %0.3f       \\t\" \n",
    "      % metrics.completeness_score(labels, km.labels_))\n",
    "print(\"V-measure: %0.3f          \\t\" \n",
    "      % metrics.v_measure_score(labels, km.labels_))\n",
    "print(\"Adjusted Rand-Index: %.3f \\t\"\n",
    "      % metrics.adjusted_rand_score(labels, km.labels_))\n",
    "print(\"Silhouette Coefficient: %0.3f \\t\"\n",
    "      % metrics.silhouette_score(X, km.labels_, sample_size=1000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "Cluster 0: com university cs article space posting just like host nntpCluster 1: god com sandvik people jesus kent say don christian moralityCluster 2: space nasa henry access digex toronto alaska pat gov shuttleCluster 3: graphics thanks files file image 3d gif format university program"
     ]
    }
   ],
   "source": [
    "if not opts.use_hashing:\n",
    "    print(\"Top terms per cluster:\")\n",
    "\n",
    "    if opts.n_components:\n",
    "        original_space_centroids = svd.inverse_transform(km.cluster_centers_)\n",
    "        order_centroids = original_space_centroids.argsort()[:, ::-1]\n",
    "    else:\n",
    "        order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "\n",
    "    terms = vectorizer.get_feature_names()\n",
    "    for i in range(true_k):\n",
    "        print(\"Cluster %d:\" % i, end='')\n",
    "        for ind in order_centroids[i, :10]:\n",
    "            print(' %s' % terms[ind], end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: [Topic extraction with NNMF & LDA](https://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html#sphx-glr-auto-examples-applications-plot-topics-extraction-with-nmf-lda-py)\n",
    "- NNMF is applied using two different objective functions:\n",
    "    - Frobenius norm\n",
    "    - generalized Kullback-Leibler divergence (equivalent to probabilistic LSA (?))\n",
    "- Default params should complete in tens of seconds. Increasing dimensionality should increase time to complexity (NNMF: polynomial: LDA: #samples * #iterations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_components = 10\n",
    "n_top_words = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1.674s.\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "data, _ = fetch_20newsgroups(shuffle=True, random_state=1,\n",
    "                             remove=('headers', 'footers', 'quotes'),\n",
    "                             return_X_y=True)\n",
    "data_samples = data[:n_samples]\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.292s.\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,\n",
    "                                   max_features=n_features,\n",
    "                                   stop_words='english')\n",
    "t0 = time()\n",
    "tfidf = tfidf_vectorizer.fit_transform(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf features for LDA...\n",
      "done in 0.299s.\n"
     ]
    }
   ],
   "source": [
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english')\n",
    "t0 = time()\n",
    "tf = tf_vectorizer.fit_transform(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.460s.\n"
     ]
    }
   ],
   "source": [
    "# fit NMF model - Frobenius norm\n",
    "t0 = time()\n",
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          alpha=.1, l1_ratio=.5).fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in NMF model (Frobenius norm):\n",
      "Topic #0: just people don think like know good time make way really say ve right want did ll new use years\n",
      "Topic #1: windows use dos using window program os application drivers help software pc running ms screen files version work code mode\n",
      "Topic #2: god jesus bible faith christian christ christians does sin heaven believe lord life mary church atheism love belief human religion\n",
      "Topic #3: thanks know does mail advance hi info interested email anybody looking card help like appreciated information list send video need\n",
      "Topic #4: car cars tires miles 00 new engine insurance price condition oil speed power good 000 brake year models used bought\n",
      "Topic #5: edu soon send com university internet mit ftp mail cc pub article information hope email mac home program blood contact\n",
      "Topic #6: file files problem format win sound ftp pub read save site image help available create copy running memory self version\n",
      "Topic #7: game team games year win play season players nhl runs goal toronto hockey division flyers player defense leafs bad won\n",
      "Topic #8: drive drives hard disk floppy software card mac computer power scsi controller apple 00 mb pc rom sale problem monitor\n",
      "Topic #9: key chip clipper keys encryption government public use secure enforcement phone nsa law communications security clinton used standard legal data\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTopics in NMF model (Frobenius norm):\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1.206s.\n"
     ]
    }
   ],
   "source": [
    "# fit NMF model - Kullback-Leibler\n",
    "t0 = time()\n",
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          beta_loss='kullback-leibler', \n",
    "          solver='mu', \n",
    "          max_iter=1000, \n",
    "          alpha=.1,\n",
    "          l1_ratio=.5).fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: people don think just right did like time say really know make said question course let way real things good\n",
      "Topic #1: windows thanks help hi using looking does info software video use dos pc advance anybody mail appreciated card need know\n",
      "Topic #2: god does jesus true book christian bible christians religion faith church believe read life christ says people lord exist say\n",
      "Topic #3: thanks know bike interested car mail new like price edu heard list hear want cars email contact just com mark\n",
      "Topic #4: 10 time year power 12 sale 15 new offer 20 30 00 16 monitor ve 11 14 condition problem 100\n",
      "Topic #5: space government 00 nasa public security states earth phone 1993 research technology university subject information science data internet provide blood\n",
      "Topic #6: edu file com program try problem files soon window remember sun win send library mike article just mit oh code\n",
      "Topic #7: game team year games play world season won case division players win nhl flyers second toronto points cubs ll al\n",
      "Topic #8: drive think hard drives disk mac apple need number software scsi computer don card floppy bus cable actually controller memory\n",
      "Topic #9: just use good like key chip got way don doesn sure clipper better going keys ll want speed encryption thought\n",
      "Fitting LDA models with tf features, n_samples=2000 and n_features=1000...\n"
     ]
    }
   ],
   "source": [
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)\n",
    "\n",
    "print(\"Fitting LDA models with tf features, \"\n",
    "      \"n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: people don think just right did like time say really know make said question course let way real things good\n",
      "Topic #1: windows thanks help hi using looking does info software video use dos pc advance anybody mail appreciated card need know\n",
      "Topic #2: god does jesus true book christian bible christians religion faith church believe read life christ says people lord exist say\n",
      "Topic #3: thanks know bike interested car mail new like price edu heard list hear want cars email contact just com mark\n",
      "Topic #4: 10 time year power 12 sale 15 new offer 20 30 00 16 monitor ve 11 14 condition problem 100\n",
      "Topic #5: space government 00 nasa public security states earth phone 1993 research technology university subject information science data internet provide blood\n",
      "Topic #6: edu file com program try problem files soon window remember sun win send library mike article just mit oh code\n",
      "Topic #7: game team year games play world season won case division players win nhl flyers second toronto points cubs ll al\n",
      "Topic #8: drive think hard drives disk mac apple need number software scsi computer don card floppy bus cable actually controller memory\n",
      "Topic #9: just use good like key chip got way don doesn sure clipper better going keys ll want speed encryption thought\n",
      "Fitting LDA models with tf features, n_samples=2000 and n_features=1000...\n"
     ]
    }
   ],
   "source": [
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)\n",
    "\n",
    "print(\"Fitting LDA models with tf features, \"\n",
    "      \"n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words Limitations\n",
    "- Unigrams can't capture phrases & multiword expressions. It also can't handle misspellings or word derivations.\n",
    "- N-Grams help solve this problem.\n",
    "- Below: corpus of two documents. The 2nd doc contains a misspelling of \"words\".\n",
    "- The vectorizer uses ```char_wb``` to vectorize look at characters inside word boundaries (whitespace)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "[[1 1 1 0 1 1 1 0]\n",
      " [1 1 0 1 1 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "ngram_vectorizer = CountVectorizer(analyzer='char_wb', \n",
    "                                   ngram_range=(2, 2))\n",
    "\n",
    "counts = ngram_vectorizer.fit_transform(['words', 'wprds'])\n",
    "\n",
    "print(ngram_vectorizer.get_feature_names() == (\n",
    "    [' w', 'ds', 'or', 'pr', 'rd', 's ', 'wo', 'wp']))\n",
    "\n",
    "# character-wise 2-gram model finds the document to match\n",
    "# in 4 of 8 documents, which may indicate a better classifier.\n",
    "\n",
    "print(counts.toarray().astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Below: ```char``` analyzer creates n-grams that span across words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# char_wb\n",
    "ngram_vectorizer = CountVectorizer(analyzer='char_wb', \n",
    "                                   ngram_range=(5, 5))\n",
    "\n",
    "ngram_vectorizer.fit_transform(['jumpy fox'])\n",
    "\n",
    "print(ngram_vectorizer.get_feature_names() == (\n",
    "    [' fox ', ' jump', 'jumpy', 'umpy ']))\n",
    "\n",
    "# char\n",
    "ngram_vectorizer = CountVectorizer(analyzer='char', \n",
    "                                   ngram_range=(5, 5))\n",
    "ngram_vectorizer.fit_transform(['jumpy fox'])\n",
    "\n",
    "print(ngram_vectorizer.get_feature_names() == (\n",
    "    ['jumpy', 'mpy f', 'py fo', 'umpy ', 'y fox']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ```char_wb``` is useful in languages that use whitespace for word separation - it generates less noise than ```chr```.\n",
    "- Bag-of-words and Bag-of-ngrams techniques destroy most of the inner structure of documents - most of the meaning is lost. Retrieving this meaning is outside the scope of scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Large text corpus vectorizing -- the hashing trick\n",
    "\n",
    "- Above vectorization techniques require in-memory mapping = not feasible for large corpus datasets.\n",
    "- [Hashing Vectorizer]() helps overcome this problem. It combines [Feature Hashing](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.FeatureHasher.html#sklearn.feature_extraction.FeatureHasher) with [Text Preprocessing & Tokenization (Count Vectorizer)](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.FeatureHasher.html#sklearn.feature_extraction.FeatureHasher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x10 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 16 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "hv = HashingVectorizer(n_features=10)\n",
    "hv.transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Real world: leave ```n_features``` at a default setting of ```2**20```. Reduce it if you see downstream memory or model size issues.\n",
    "- Dimensionality does not affect training time of algorithms that use **CSR** matrices (LinearSVC(dual=True), Perceptron, SGDClassifier, PassiveAggressive), but does for algorithms that use **CSC** matrices (LinearSVC(dual=False), Lasso(), ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x1048576 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 19 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# defaults - no collisions but much larger output space\n",
    "hv = HashingVectorizer()\n",
    "hv.transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out-of-core scaling\n",
    "- HashingVectorizer can do out-of-core scaling using a mini-batch approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom vectorizer classes\n",
    "- ```preprocessor```: a function that transforms an entire document as a single string. This could be used to strip HTML, etc.\n",
    "- ```tokenizer```: a function that splits the preprocessor's output into a list of tokens.\n",
    "- ```analyzer```: a function that replaces the preprocessor & tokenizer - custom analyzers do this for you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_tokenizer(s):\n",
    "    return s.split()\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer=my_tokenizer)\n",
    "vectorizer.build_analyzer()(u\"Some... punctuation!\") == (\n",
    "    ['some...', 'punctuation!'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips:\n",
    "- If documents are pre-tokenized by an external package, store them in files with tokens separated by whitespace. Pass ```analyzer=str.split```.\n",
    "- Advanced analysis (stemming, lemmatizing, ...) are not included in scikit-learn but can be added using custom tokenizers or analyzers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "class LemmaTokenizer:\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
    "\n",
    "vect = CountVectorizer(tokenizer=LemmaTokenizer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['color', 'color']\n"
     ]
    }
   ],
   "source": [
    "# transform British spelling to American spelling\n",
    "import re\n",
    "def to_british(tokens):\n",
    "    for t in tokens:\n",
    "        t = re.sub(r\"(...)our$\", r\"\\1or\", t)\n",
    "        t = re.sub(r\"([bt])re$\", r\"\\1er\", t)\n",
    "        t = re.sub(r\"([iy])s(e$|ing|ation)\", r\"\\1z\\2\", t)\n",
    "        t = re.sub(r\"ogue$\", \"og\", t)\n",
    "        yield t\n",
    "\n",
    "class CustomVectorizer(CountVectorizer):\n",
    "    def build_tokenizer(self):\n",
    "        tokenize = super().build_tokenizer()\n",
    "        return lambda doc: list(to_british(tokenize(doc)))\n",
    "\n",
    "print(CustomVectorizer().build_analyzer()(u\"color colour\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
