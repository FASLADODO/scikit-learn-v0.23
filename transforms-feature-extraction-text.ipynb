{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Feature Extraction - Text](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer)\n",
    "- Scikit provides text-to-numerical feature extraction utilities to aid *tokenization*, *counting* and *normalizing* functions.\n",
    "- A document corpus is defined as a matrix with one row per document and one column per token (word) that occurs in the corpus.\n",
    "- Most corpus examples use small subsets of the total words available, so the resulting matrix will be very sparse. Scikit uses ```scipy.sparse``` to aid computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x9 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 19 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CountVectorizer handles tokenization & occurrence counting.\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "# tokenize & count word occurrences\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# default configuration: extract words of at least 2 letters.\n",
    "analyze = vectorizer.build_analyzer()\n",
    "analyze(\"This is a text document to analyze.\") == (\n",
    "    ['this', 'is', 'text', 'document', 'to', 'analyze'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 0, 1, 0, 1],\n",
       "       [0, 1, 0, 1, 0, 2, 1, 0, 1],\n",
       "       [1, 0, 0, 0, 1, 0, 1, 1, 0],\n",
       "       [0, 1, 1, 1, 0, 0, 1, 0, 1]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names() == (\n",
    "    ['and', 'document', 'first', 'is', 'one',\n",
    "     'second', 'the', 'third', 'this'])\n",
    "\n",
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feature name : column index map is stored in vocabulary_\n",
    "vectorizer.vocabulary_.get('document')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# words not seen in training corpus will be ignored.\n",
    "vectorizer.transform(['Something completely new.']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# previous corpus: 1st & last docs = same #words, \n",
    "# so encoded into equal vectors.\n",
    "# we lose track of last doc being a question.\n",
    "# to preserve some local ordering info, we can use 2-grams.\n",
    "\n",
    "bigram_vectorizer = CountVectorizer(ngram_range=(1, 2),\n",
    "                                    token_pattern=r'\\b\\w+\\b', min_df=1)\n",
    "analyze = bigram_vectorizer.build_analyzer()\n",
    "analyze('Bi-grams are cool!') == (\n",
    "    ['bi', 'grams', 'are', 'cool', 'bi grams', 'grams are', 'are cool'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0],\n",
       "       [0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0],\n",
       "       [0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this vectorizer has a much bigger array.\n",
    "X_2 = bigram_vectorizer.fit_transform(corpus).toarray()\n",
    "X_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we can check for the question phrase..\n",
    "feature_index = bigram_vectorizer.vocabulary_.get('is this')\n",
    "X_2[:, feature_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop Words\n",
    "- Presumably uninformative & okay to ignore, but not always\n",
    "- scikit-learn std stop words list not ideal. See [NQY18](https://scikit-learn.org/stable/modules/feature_extraction.html#id5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tf-Idf term weighting\n",
    "- defined as $\\text{tf-idf(t,d)}=\\text{tf(t,d)} \\times \\text{idf(t)}$\n",
    "- where $\\text{idf}(t) = \\log{\\frac{1 + n}{1+\\text{df}(t)}} + 1$\n",
    "- n = #documents in corpus\n",
    "- df(t) = #documents in corpus containing term _t_\n",
    "- resulting tf-idf vectors are then Euclidean-normalized: $v_{norm} = \\frac{v}{||v||_2} = \\frac{v}{\\sqrt{v{_1}^2 +\n",
    "v{_2}^2 + \\dots + v{_n}^2}}$\n",
    "\n",
    "- scikit-learn implementation of idf differs slightly from std textbook definition. The \"1\" count is added to the idf instead of the idf denominator when ```smooth_idf=False```.\n",
    "    - textbook: $\\text{idf}(t) = \\log{\\frac{n}{1+\\text{df}(t)}}.$\n",
    "    - scikit:   $\\text{idf}(t) = \\log{\\frac{n}{\\text{df}(t)}} + 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer(smooth_idf=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transformer = TfidfTransformer(smooth_idf=False)\n",
    "transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2)\t0.5732079309279059\n",
      "  (0, 0)\t0.8194099510753754\n",
      "  (1, 0)\t1.0\n",
      "  (2, 0)\t1.0\n",
      "  (3, 0)\t1.0\n",
      "  (4, 1)\t0.8808994832762984\n",
      "  (4, 0)\t0.47330339145578754\n",
      "  (5, 2)\t0.8135516873095774\n",
      "  (5, 0)\t0.5814926070688599\n",
      "[[0.81940995 0.         0.57320793]\n",
      " [1.         0.         0.        ]\n",
      " [1.         0.         0.        ]\n",
      " [1.         0.         0.        ]\n",
      " [0.47330339 0.88089948 0.        ]\n",
      " [0.58149261 0.         0.81355169]]\n"
     ]
    }
   ],
   "source": [
    "# first term present 1005 of the time = not too interesting.\n",
    "# 2nd, 3rd terms less so = probably more interesting.\n",
    "counts = [[3, 0, 1],\n",
    "          [2, 0, 0],\n",
    "          [3, 0, 0],\n",
    "          [4, 0, 0],\n",
    "          [3, 2, 0],\n",
    "          [3, 0, 2]]\n",
    "\n",
    "tfidf = transformer.fit_transform(counts)\n",
    "print(tfidf)\n",
    "print(tfidf.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Tf-idf Vectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer) - combines [Count Vectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer) and [Tf-idf Transformer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer) into a single model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x9 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 19 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- While Tf-Idf normalization is useful, there are use cases where binary markers offer better information. Use the ```binary``` parameter of CountVectorizer for this.\n",
    "- For example, Bernoulli NB models discrete boolean random variables.\n",
    "- Also: very short texts have very noisy tf-idf values - the binary version can be more stable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding text files\n",
    "- Text is characters; files are bytes \n",
    "- Common formats: ASCII, Latin-1, KOI8-R, UTF-8, UTF-16, ...\n",
    "- CountVectorizer uses default ```encoding=\"utf-8``` for encoding.\n",
    "- Below: using ```chardet``` to find encoding of three texts, then using CountVectorizer to vectorize them & print out the learned vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sei': 15, 'mir': 13, 'gegrüßt': 6, 'mein': 12, 'sauerkraut': 14, 'holdselig': 10, 'sind': 16, 'deine': 1, 'gerüche': 7, 'auf': 0, 'flügeln': 4, 'des': 2, 'gesanges': 8, 'herzliebchen': 9, 'trag': 17, 'ich': 11, 'dich': 3, 'fort': 5}\n",
      "{'sei': 15, 'mir': 13, 'gegrüßt': 6, 'mein': 12, 'sauerkraut': 14, 'holdselig': 10, 'sind': 16, 'deine': 1, 'gerüche': 7, 'auf': 0, 'flügeln': 4, 'des': 2, 'gesanges': 8, 'herzliebchen': 9, 'trag': 17, 'ich': 11, 'dich': 3, 'fort': 5}\n",
      "{'sei': 15, 'mir': 13, 'gegrüßt': 6, 'mein': 12, 'sauerkraut': 14, 'holdselig': 10, 'sind': 16, 'deine': 1, 'gerüche': 7, 'auf': 0, 'flügeln': 4, 'des': 2, 'gesanges': 8, 'herzliebchen': 9, 'trag': 17, 'ich': 11, 'dich': 3, 'fort': 5}\n",
      "{'sei': 15, 'mir': 13, 'gegrüßt': 6, 'mein': 12, 'sauerkraut': 14, 'holdselig': 10, 'sind': 16, 'deine': 1, 'gerüche': 7, 'auf': 0, 'flügeln': 4, 'des': 2, 'gesanges': 8, 'herzliebchen': 9, 'trag': 17, 'ich': 11, 'dich': 3, 'fort': 5}\n",
      "{'sei': 15, 'mir': 13, 'gegrüßt': 6, 'mein': 12, 'sauerkraut': 14, 'holdselig': 10, 'sind': 16, 'deine': 1, 'gerüche': 7, 'auf': 0, 'flügeln': 4, 'des': 2, 'gesanges': 8, 'herzliebchen': 9, 'trag': 17, 'ich': 11, 'dich': 3, 'fort': 5}\n",
      "{'sei': 15, 'mir': 13, 'gegrüßt': 6, 'mein': 12, 'sauerkraut': 14, 'holdselig': 10, 'sind': 16, 'deine': 1, 'gerüche': 7, 'auf': 0, 'flügeln': 4, 'des': 2, 'gesanges': 8, 'herzliebchen': 9, 'trag': 17, 'ich': 11, 'dich': 3, 'fort': 5}\n",
      "{'sei': 15, 'mir': 13, 'gegrüßt': 6, 'mein': 12, 'sauerkraut': 14, 'holdselig': 10, 'sind': 16, 'deine': 1, 'gerüche': 7, 'auf': 0, 'flügeln': 4, 'des': 2, 'gesanges': 8, 'herzliebchen': 9, 'trag': 17, 'ich': 11, 'dich': 3, 'fort': 5}\n",
      "{'sei': 15, 'mir': 13, 'gegrüßt': 6, 'mein': 12, 'sauerkraut': 14, 'holdselig': 10, 'sind': 16, 'deine': 1, 'gerüche': 7, 'auf': 0, 'flügeln': 4, 'des': 2, 'gesanges': 8, 'herzliebchen': 9, 'trag': 17, 'ich': 11, 'dich': 3, 'fort': 5}\n",
      "{'sei': 15, 'mir': 13, 'gegrüßt': 6, 'mein': 12, 'sauerkraut': 14, 'holdselig': 10, 'sind': 16, 'deine': 1, 'gerüche': 7, 'auf': 0, 'flügeln': 4, 'des': 2, 'gesanges': 8, 'herzliebchen': 9, 'trag': 17, 'ich': 11, 'dich': 3, 'fort': 5}\n",
      "{'sei': 15, 'mir': 13, 'gegrüßt': 6, 'mein': 12, 'sauerkraut': 14, 'holdselig': 10, 'sind': 16, 'deine': 1, 'gerüche': 7, 'auf': 0, 'flügeln': 4, 'des': 2, 'gesanges': 8, 'herzliebchen': 9, 'trag': 17, 'ich': 11, 'dich': 3, 'fort': 5}\n",
      "{'sei': 15, 'mir': 13, 'gegrüßt': 6, 'mein': 12, 'sauerkraut': 14, 'holdselig': 10, 'sind': 16, 'deine': 1, 'gerüche': 7, 'auf': 0, 'flügeln': 4, 'des': 2, 'gesanges': 8, 'herzliebchen': 9, 'trag': 17, 'ich': 11, 'dich': 3, 'fort': 5}\n",
      "{'sei': 15, 'mir': 13, 'gegrüßt': 6, 'mein': 12, 'sauerkraut': 14, 'holdselig': 10, 'sind': 16, 'deine': 1, 'gerüche': 7, 'auf': 0, 'flügeln': 4, 'des': 2, 'gesanges': 8, 'herzliebchen': 9, 'trag': 17, 'ich': 11, 'dich': 3, 'fort': 5}\n",
      "{'sei': 15, 'mir': 13, 'gegrüßt': 6, 'mein': 12, 'sauerkraut': 14, 'holdselig': 10, 'sind': 16, 'deine': 1, 'gerüche': 7, 'auf': 0, 'flügeln': 4, 'des': 2, 'gesanges': 8, 'herzliebchen': 9, 'trag': 17, 'ich': 11, 'dich': 3, 'fort': 5}\n",
      "{'sei': 15, 'mir': 13, 'gegrüßt': 6, 'mein': 12, 'sauerkraut': 14, 'holdselig': 10, 'sind': 16, 'deine': 1, 'gerüche': 7, 'auf': 0, 'flügeln': 4, 'des': 2, 'gesanges': 8, 'herzliebchen': 9, 'trag': 17, 'ich': 11, 'dich': 3, 'fort': 5}\n",
      "{'sei': 15, 'mir': 13, 'gegrüßt': 6, 'mein': 12, 'sauerkraut': 14, 'holdselig': 10, 'sind': 16, 'deine': 1, 'gerüche': 7, 'auf': 0, 'flügeln': 4, 'des': 2, 'gesanges': 8, 'herzliebchen': 9, 'trag': 17, 'ich': 11, 'dich': 3, 'fort': 5}\n",
      "{'sei': 15, 'mir': 13, 'gegrüßt': 6, 'mein': 12, 'sauerkraut': 14, 'holdselig': 10, 'sind': 16, 'deine': 1, 'gerüche': 7, 'auf': 0, 'flügeln': 4, 'des': 2, 'gesanges': 8, 'herzliebchen': 9, 'trag': 17, 'ich': 11, 'dich': 3, 'fort': 5}\n",
      "{'sei': 15, 'mir': 13, 'gegrüßt': 6, 'mein': 12, 'sauerkraut': 14, 'holdselig': 10, 'sind': 16, 'deine': 1, 'gerüche': 7, 'auf': 0, 'flügeln': 4, 'des': 2, 'gesanges': 8, 'herzliebchen': 9, 'trag': 17, 'ich': 11, 'dich': 3, 'fort': 5}\n",
      "{'sei': 15, 'mir': 13, 'gegrüßt': 6, 'mein': 12, 'sauerkraut': 14, 'holdselig': 10, 'sind': 16, 'deine': 1, 'gerüche': 7, 'auf': 0, 'flügeln': 4, 'des': 2, 'gesanges': 8, 'herzliebchen': 9, 'trag': 17, 'ich': 11, 'dich': 3, 'fort': 5}\n"
     ]
    }
   ],
   "source": [
    "import chardet    # doctest: +SKIP\n",
    "text1 = b\"Sei mir gegr\\xc3\\xbc\\xc3\\x9ft mein Sauerkraut\"\n",
    "text2 = b\"holdselig sind deine Ger\\xfcche\"\n",
    "text3 = b\"\\xff\\xfeA\\x00u\\x00f\\x00 \\x00F\\x00l\\x00\\xfc\\x00g\\x00e\\x00l\\x00n\\x00 \\x00d\\x00e\\x00s\\x00 \\x00G\\x00e\\x00s\\x00a\\x00n\\x00g\\x00e\\x00s\\x00,\\x00 \\x00H\\x00e\\x00r\\x00z\\x00l\\x00i\\x00e\\x00b\\x00c\\x00h\\x00e\\x00n\\x00,\\x00 \\x00t\\x00r\\x00a\\x00g\\x00 \\x00i\\x00c\\x00h\\x00 \\x00d\\x00i\\x00c\\x00h\\x00 \\x00f\\x00o\\x00r\\x00t\\x00\"\n",
    "decoded = [x.decode(chardet.detect(x)['encoding'])\n",
    "           for x in (text1, text2, text3)]        # doctest: +SKIP\n",
    "v = CountVectorizer().fit(decoded).vocabulary_    # doctest: +SKIP\n",
    "for term in v: print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications\n",
    "- Supervised: use with linear models to build document classifiers\n",
    "- Unsupervised: used with clustering models to group similar documents.\n",
    "- Learn main topics using NNMF & LDA.\n",
    "- Examples below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: [Text doc classification - sparse features](https://scikit-learn.org/stable/auto_examples/text/plot_document_classification_20newsgroups.html#sphx-glr-auto-examples-text-plot-document-classification-20newsgroups-py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "from optparse import OptionParser\n",
    "import sys\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, ComplementNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils.extmath import density\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display progress logs on stdout\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s %(levelname)s %(message)s')\n",
    "\n",
    "op = OptionParser()\n",
    "op.add_option(\"--report\",\n",
    "              action=\"store_true\", dest=\"print_report\",\n",
    "              help=\"Print a detailed classification report.\")\n",
    "op.add_option(\"--chi2_select\",\n",
    "              action=\"store\", type=\"int\", dest=\"select_chi2\",\n",
    "              help=\"Select some number of features using a chi-squared test\")\n",
    "op.add_option(\"--confusion_matrix\",\n",
    "              action=\"store_true\", dest=\"print_cm\",\n",
    "              help=\"Print the confusion matrix.\")\n",
    "op.add_option(\"--top10\",\n",
    "              action=\"store_true\", dest=\"print_top10\",\n",
    "              help=\"Print ten most discriminative terms per class\"\n",
    "                   \" for every classifier.\")\n",
    "op.add_option(\"--all_categories\",\n",
    "              action=\"store_true\", dest=\"all_categories\",\n",
    "              help=\"Whether to use all categories or not.\")\n",
    "op.add_option(\"--use_hashing\",\n",
    "              action=\"store_true\",\n",
    "              help=\"Use a hashing vectorizer.\")\n",
    "op.add_option(\"--n_features\",\n",
    "              action=\"store\", type=int, default=2 ** 16,\n",
    "              help=\"n_features when using the hashing vectorizer.\")\n",
    "op.add_option(\"--filtered\",\n",
    "              action=\"store_true\",\n",
    "              help=\"Remove newsgroup information that is easily overfit: \"\n",
    "                   \"headers, signatures, and quoting.\")\n",
    "\n",
    "\n",
    "def is_interactive():\n",
    "    return not hasattr(sys.modules['__main__'], '__file__')\n",
    "\n",
    "# work-around for Jupyter notebook and IPython console\n",
    "argv = [] if is_interactive() else sys.argv[1:]\n",
    "(opts, args) = op.parse_args(argv)\n",
    "if len(args) > 0:\n",
    "    op.error(\"this script takes no arguments.\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 20 newsgroups dataset for categories:\n",
      "['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n"
     ]
    }
   ],
   "source": [
    "if opts.all_categories:\n",
    "    categories = None\n",
    "else:\n",
    "    categories = [\n",
    "        'alt.atheism',\n",
    "        'talk.religion.misc',\n",
    "        'comp.graphics',\n",
    "        'sci.space',\n",
    "    ]\n",
    "\n",
    "if opts.filtered:\n",
    "    remove = ('headers', 'footers', 'quotes')\n",
    "else:\n",
    "    remove = ()\n",
    "\n",
    "print(\"Loading 20 newsgroups dataset for categories:\")\n",
    "print(categories if categories else \"all\")\n",
    "\n",
    "data_train = fetch_20newsgroups(subset='train', \n",
    "                                categories=categories,\n",
    "                                shuffle=True, \n",
    "                                random_state=42,\n",
    "                                remove=remove)\n",
    "\n",
    "data_test = fetch_20newsgroups(subset='test', \n",
    "                               categories=categories,\n",
    "                               shuffle=True, \n",
    "                               random_state=42,\n",
    "                               remove=remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2034 documents - 3.980MB (training set)\n",
      "1353 documents - 2.867MB (test set)\n",
      "4 categories\n"
     ]
    }
   ],
   "source": [
    "target_names = data_train.target_names\n",
    "\n",
    "def size_mb(docs):\n",
    "    return sum(len(s.encode('utf-8')) for s in docs) / 1e6\n",
    "\n",
    "data_train_size_mb = size_mb(data_train.data)\n",
    "data_test_size_mb  = size_mb(data_test.data)\n",
    "\n",
    "print(\"%d documents - %0.3fMB (training set)\" % (\n",
    "    len(data_train.data), data_train_size_mb))\n",
    "print(\"%d documents - %0.3fMB (test set)\" % (\n",
    "    len(data_test.data), data_test_size_mb))\n",
    "print(\"%d categories\" % len(target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features\n",
      "done in 0.431254s at 9.228MB/s\n",
      "n_samples: 2034, n_features: 33809\n"
     ]
    }
   ],
   "source": [
    "y_train, y_test = data_train.target, data_test.target\n",
    "\n",
    "print(\"Extracting features\")\n",
    "t0 = time()\n",
    "if opts.use_hashing:\n",
    "    vectorizer = HashingVectorizer(stop_words     = 'english', \n",
    "                                   alternate_sign = False,\n",
    "                                   n_features     = opts.n_features)\n",
    "    X_train = vectorizer.transform(data_train.data)\n",
    "else:\n",
    "    vectorizer = TfidfVectorizer(sublinear_tf     = True, \n",
    "                                 max_df           = 0.5,\n",
    "                                 stop_words       = 'english')\n",
    "    X_train = vectorizer.fit_transform(data_train.data)\n",
    "    \n",
    "duration = time() - t0\n",
    "print(\"done in %fs at %0.3fMB/s\" % (duration, \n",
    "                                    data_train_size_mb / duration))\n",
    "print(\"n_samples: %d, n_features: %d\" % X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features\n",
      "done in 0.262803s at 10.911MB/s\n",
      "n_samples: 1353, n_features: 33809\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracting features\")\n",
    "t0 = time()\n",
    "X_test = vectorizer.transform(data_test.data)\n",
    "duration = time() - t0\n",
    "print(\"done in %fs at %0.3fMB/s\" % (duration, \n",
    "                                    data_test_size_mb / duration))\n",
    "print(\"n_samples: %d, n_features: %d\" % X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "if opts.use_hashing:\n",
    "    feature_names = None\n",
    "else:\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "if opts.select_chi2:\n",
    "    print(\"Extracting %d best features: chi-squared test\" %\n",
    "          opts.select_chi2)\n",
    "    \n",
    "    t0 = time()\n",
    "    ch2 = SelectKBest(chi2, k=opts.select_chi2)\n",
    "    X_train = ch2.fit_transform(X_train, y_train)\n",
    "    X_test = ch2.transform(X_test)\n",
    "    \n",
    "    if feature_names:\n",
    "        feature_names = [feature_names[i] for i\n",
    "                         in ch2.get_support(indices=True)]\n",
    "    print(\"done in %fs\" % (time() - t0))\n",
    "\n",
    "if feature_names:\n",
    "    feature_names = np.asarray(feature_names)\n",
    "\n",
    "def trim(s):\n",
    "    return s if len(s) <= 80 else s[:77] + \"...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Classifier\n",
      "Perceptron\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bjpcjp/.local/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:556: UserWarning: \"sag\" solver requires many iterations to fit an intercept with sparse inputs. Either set the solver to \"auto\" or \"sparse_cg\", or set a low \"tol\" and a high \"max_iter\" (especially if inputs are not standardized).\n",
      "  '\"sag\" solver requires many iterations to fit '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passive-Aggressive\n",
      "kNN\n",
      "Random forest\n",
      "Elastic-Net penalty\n",
      "NearestCentroid (aka Rocchio classifier)\n",
      "Naive Bayes\n",
      "LinearSVC with L1-based feature selection\n"
     ]
    }
   ],
   "source": [
    "# benchmark 15 classifiers\n",
    "def benchmark(clf):\n",
    "    #print(\"Training: \")\n",
    "    #print(clf)\n",
    "    t0 = time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_time = time() - t0\n",
    "    #print(\"train time: %0.3fs\" % train_time)\n",
    "\n",
    "    t0 = time()\n",
    "    pred = clf.predict(X_test)\n",
    "    test_time = time() - t0\n",
    "    #print(\"test time:  %0.3fs\" % test_time)\n",
    "\n",
    "    score = metrics.accuracy_score(y_test, pred)\n",
    "    #print(\"accuracy:   %0.3f\" % score)\n",
    "\n",
    "    if hasattr(clf, 'coef_'):\n",
    "        #print(\"dimensionality: %d\" % clf.coef_.shape[1])\n",
    "        #print(\"density: %f\" % density(clf.coef_))\n",
    "\n",
    "        if opts.print_top10 and feature_names is not None:\n",
    "            #print(\"top 10 keywords per class:\")\n",
    "            for i, label in enumerate(target_names):\n",
    "                top10 = np.argsort(clf.coef_[i])[-10:]\n",
    "                #print(trim(\"%s: %s\" % (label, \" \".join(feature_names[top10]))))\n",
    "\n",
    "    #if opts.print_report:\n",
    "        #print(\"classification report:\")\n",
    "        #print(metrics.classification_report(y_test, pred, target_names=target_names))\n",
    "    #if opts.print_cm:\n",
    "        #print(\"confusion matrix:\")\n",
    "        #print(metrics.confusion_matrix(y_test, pred))\n",
    "\n",
    "    clf_descr = str(clf).split('(')[0]\n",
    "    return clf_descr, score, train_time, test_time\n",
    "\n",
    "\n",
    "results = []\n",
    "for clf, name in (\n",
    "        (RidgeClassifier(tol=1e-2, \n",
    "                         solver=\"sag\"), \n",
    "         \"Ridge Classifier\"),\n",
    "    \n",
    "        (Perceptron(max_iter=50), \n",
    "         \"Perceptron\"),\n",
    "    \n",
    "        (PassiveAggressiveClassifier(max_iter=50),\n",
    "         \"Passive-Aggressive\"),\n",
    "    \n",
    "        (KNeighborsClassifier(n_neighbors=10), \n",
    "         \"kNN\"),\n",
    "    \n",
    "        (RandomForestClassifier(), \n",
    "         \"Random forest\")):\n",
    "    print(name)\n",
    "    results.append(benchmark(clf))\n",
    "\n",
    "for penalty in [\"l2\", \"l1\"]:\n",
    "    #print(\"%s penalty\" % penalty.upper())\n",
    "    # Train Liblinear model\n",
    "    results.append(benchmark(LinearSVC(penalty=penalty, dual=False,\n",
    "                                       tol=1e-3)))\n",
    "\n",
    "    # Train SGD model\n",
    "    results.append(benchmark(SGDClassifier(alpha=.0001, max_iter=50,\n",
    "                                           penalty=penalty)))\n",
    "\n",
    "# Train SGD with Elastic Net penalty\n",
    "print(\"Elastic-Net penalty\")\n",
    "results.append(benchmark(SGDClassifier(alpha=.0001, max_iter=50,\n",
    "                                       penalty=\"elasticnet\")))\n",
    "\n",
    "# Train NearestCentroid without threshold\n",
    "print(\"NearestCentroid (aka Rocchio classifier)\")\n",
    "results.append(benchmark(NearestCentroid()))\n",
    "\n",
    "# Train sparse Naive Bayes classifiers (3)\n",
    "print(\"Naive Bayes\")\n",
    "results.append(benchmark(MultinomialNB(alpha=.01)))\n",
    "results.append(benchmark(BernoulliNB(alpha=.01)))\n",
    "results.append(benchmark(ComplementNB(alpha=.1)))\n",
    "\n",
    "print(\"LinearSVC with L1-based feature selection\")\n",
    "# smaller C = stronger regularization = more sparsity\n",
    "results.append(benchmark(Pipeline([\n",
    "  ('feature_selection', SelectFromModel(LinearSVC(penalty=\"l1\", \n",
    "                                                  dual=False,\n",
    "                                                  tol=1e-3))),\n",
    "  ('classification', LinearSVC(penalty=\"l2\"))])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plots\n",
    "indices = np.arange(len(results))\n",
    "\n",
    "results = [[x[i] for x in results] for i in range(4)]\n",
    "\n",
    "clf_names, score, training_time, test_time = results\n",
    "training_time = np.array(training_time) / np.max(training_time)\n",
    "test_time = np.array(test_time) / np.max(test_time)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.title(\"Score\")\n",
    "plt.barh(indices, score, .2, label=\"score\", color='navy')\n",
    "plt.barh(indices + .3, training_time, .2, label=\"training time\",\n",
    "         color='c')\n",
    "plt.barh(indices + .6, test_time, .2, label=\"test time\", color='darkorange')\n",
    "plt.yticks(())\n",
    "plt.legend(loc='best')\n",
    "plt.subplots_adjust(left=.25)\n",
    "plt.subplots_adjust(top=.95)\n",
    "plt.subplots_adjust(bottom=.05)\n",
    "\n",
    "for i, c in zip(indices, clf_names):\n",
    "    plt.text(-.3, i, c)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: [Text doc clustering](https://scikit-learn.org/stable/auto_examples/text/plot_document_clustering.html#sphx-glr-auto-examples-text-plot-document-clustering-py)\n",
    "- Clusters documents by topics using bag-of-words.\n",
    "- Two feature extractors demo'd:\n",
    "    - **TfidfVectorizer**: uses in-memory vocabulary (Python dict) to map words to feature indices, find a word frequency matrix, then re-weight the matrix using an inverse doc frequency (IDF) vector.\n",
    "    - **HasingVectorizer**: hashes words to word counts in a fixed dimensional space (collisions are possible), normalizes the counts to have an L2-norm equal to one (projected to a Euclidean unit-ball). Does *not* provide IDF weighting - pipeline the output to a Transformer instance if it is needed.\n",
    "\n",
    "- K-means and it scalable minibatch variant are used for comparison.\n",
    "\n",
    "- LSA (latent semantic analysis can be used to reduce dimensionality.\n",
    "\n",
    "- K-means is sensitive to feature scaling - IDF weighting helps to improve cluster quality.\n",
    "\n",
    "- K-means is optimizing a non-convex function - it is likely to end up in a local optimium. Plan on using multiple runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "import logging\n",
    "from optparse import OptionParser\n",
    "import sys\n",
    "from time import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s %(levelname)s %(message)s')\n",
    "\n",
    "op = OptionParser()\n",
    "op.add_option(\"--lsa\",\n",
    "              dest=\"n_components\", \n",
    "              type=\"int\",\n",
    "              help=\"Preprocess documents with LSA.\")\n",
    "op.add_option(\"--no-minibatch\",\n",
    "              action=\"store_false\", \n",
    "              dest=\"minibatch\", \n",
    "              default=True,\n",
    "              help=\"Use ordinary k-means\")\n",
    "op.add_option(\"--no-idf\",\n",
    "              action=\"store_false\", \n",
    "              dest=\"use_idf\", \n",
    "              default=True,\n",
    "              help=\"Disable IDF feature weighting.\")\n",
    "op.add_option(\"--use-hashing\",\n",
    "              action=\"store_true\", \n",
    "              default=False,\n",
    "              help=\"Use hashing feature vectorizer\")\n",
    "op.add_option(\"--n-features\", \n",
    "              type=int, \n",
    "              default=10000,\n",
    "              help=\"Maximum #features (dimensions) to extract\")\n",
    "op.add_option(\"--verbose\",\n",
    "              action=\"store_true\", \n",
    "              dest=\"verbose\", \n",
    "              default=False,\n",
    "              help=\"Print progress reports inside k-means.\")\n",
    "\n",
    "def is_interactive():\n",
    "    return not hasattr(sys.modules['__main__'], '__file__')\n",
    "\n",
    "argv = [] if is_interactive() else sys.argv[1:]\n",
    "(opts, args) = op.parse_args(argv)\n",
    "if len(args) > 0:\n",
    "    op.error(\"this script takes no arguments.\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3387 documents\n",
      "4 categories\n"
     ]
    }
   ],
   "source": [
    "categories = [\n",
    "    'alt.atheism',\n",
    "    'talk.religion.misc',\n",
    "    'comp.graphics',\n",
    "    'sci.space',\n",
    "]\n",
    "# Uncomment the following to do the analysis on all the categories\n",
    "# categories = None\n",
    "dataset = fetch_20newsgroups(subset='all', \n",
    "                             categories=categories,\n",
    "                             shuffle=True, \n",
    "                             random_state=42)\n",
    "\n",
    "print(\"%d documents\" % len(dataset.data))\n",
    "print(\"%d categories\" % len(dataset.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.644545s\n",
      "n_samples: 3387, n_features: 10000\n"
     ]
    }
   ],
   "source": [
    "labels = dataset.target\n",
    "true_k = np.unique(labels).shape[0]\n",
    "\n",
    "t0 = time()\n",
    "if opts.use_hashing:\n",
    "    if opts.use_idf:\n",
    "        hasher = HashingVectorizer(n_features     = opts.n_features,\n",
    "                                   stop_words     = 'english', \n",
    "                                   alternate_sign = False,\n",
    "                                   norm           = None)\n",
    "        vectorizer = make_pipeline(hasher, \n",
    "                                   TfidfTransformer())\n",
    "    else:\n",
    "        vectorizer = HashingVectorizer(n_features     = opts.n_features,\n",
    "                                       stop_words     = 'english',\n",
    "                                       alternate_sign = False, \n",
    "                                       norm           = 'l2')\n",
    "else:\n",
    "    vectorizer = TfidfVectorizer(max_df       = 0.5, \n",
    "                                 max_features = opts.n_features,\n",
    "                                 min_df       = 2, \n",
    "                                 stop_words   = 'english',\n",
    "                                 use_idf      = opts.use_idf)\n",
    "X = vectorizer.fit_transform(dataset.data)\n",
    "\n",
    "print(\"done in %fs\" % (time() - t0))\n",
    "print(\"n_samples: %d, n_features: %d\" % X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "if opts.n_components:\n",
    "    print(\"Performing dimensionality reduction using LSA\")\n",
    "    t0 = time()\n",
    "    # Vectorizer results are normalized, which makes KMeans behave as\n",
    "    # spherical k-means for better results. Since LSA/SVD results are\n",
    "    # not normalized, we have to redo the normalization.\n",
    "    svd = TruncatedSVD(opts.n_components)\n",
    "    normalizer = Normalizer(copy=False)\n",
    "    lsa = make_pipeline(svd, normalizer)\n",
    "\n",
    "    X = lsa.fit_transform(X)\n",
    "\n",
    "    print(\"done in %fs\" % (time() - t0))\n",
    "\n",
    "    explained_variance = svd.explained_variance_ratio_.sum()\n",
    "    print(\"Explained variance of the SVD step: {}%\".format(\n",
    "        int(explained_variance * 100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering sparse data with MiniBatchKMeans(batch_size=1000, init_size=1000, n_clusters=4, n_init=1,\n",
      "                verbose=False)\n",
      "done in 0.089s            \t\n",
      "Homogeneity: 0.559        \t\n",
      "Completeness: 0.568       \t\n",
      "V-measure: 0.564          \t\n",
      "Adjusted Rand-Index: 0.548 \t\n",
      "Silhouette Coefficient: 0.007 \t\n"
     ]
    }
   ],
   "source": [
    "if opts.minibatch:\n",
    "    km = MiniBatchKMeans(n_clusters=true_k, init='k-means++', n_init=1,\n",
    "                         init_size=1000, batch_size=1000, verbose=opts.verbose)\n",
    "else:\n",
    "    km = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1,\n",
    "                verbose=opts.verbose)\n",
    "\n",
    "print(\"Clustering sparse data with %s\" % km)\n",
    "t0 = time()\n",
    "km.fit(X)\n",
    "print(\"done in %0.3fs            \\t\" \n",
    "      % (time() - t0))\n",
    "print(\"Homogeneity: %0.3f        \\t\" \n",
    "      % metrics.homogeneity_score(labels, km.labels_))\n",
    "print(\"Completeness: %0.3f       \\t\" \n",
    "      % metrics.completeness_score(labels, km.labels_))\n",
    "print(\"V-measure: %0.3f          \\t\" \n",
    "      % metrics.v_measure_score(labels, km.labels_))\n",
    "print(\"Adjusted Rand-Index: %.3f \\t\"\n",
    "      % metrics.adjusted_rand_score(labels, km.labels_))\n",
    "print(\"Silhouette Coefficient: %0.3f \\t\"\n",
    "      % metrics.silhouette_score(X, km.labels_, sample_size=1000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "Cluster 0: keith morality objective people say moral religion don think mathewCluster 1: space nasa access henry digex toronto pat gov alaska comCluster 2: god com sandvik jesus kent article people koresh apple sgiCluster 3: graphics university image thanks com posting host nntp file ac"
     ]
    }
   ],
   "source": [
    "if not opts.use_hashing:\n",
    "    print(\"Top terms per cluster:\")\n",
    "\n",
    "    if opts.n_components:\n",
    "        original_space_centroids = svd.inverse_transform(km.cluster_centers_)\n",
    "        order_centroids = original_space_centroids.argsort()[:, ::-1]\n",
    "    else:\n",
    "        order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "\n",
    "    terms = vectorizer.get_feature_names()\n",
    "    for i in range(true_k):\n",
    "        print(\"Cluster %d:\" % i, end='')\n",
    "        for ind in order_centroids[i, :10]:\n",
    "            print(' %s' % terms[ind], end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: [Topic extraction with NNMF & LDA](https://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html#sphx-glr-auto-examples-applications-plot-topics-extraction-with-nmf-lda-py)\n",
    "- NNMF is applied using two different objective functions:\n",
    "    - Frobenius norm\n",
    "    - generalized Kullback-Leibler divergence (equivalent to probabilistic LSA (?))\n",
    "- Default params should complete in tens of seconds. Increasing dimensionality should increase time to complexity (NNMF: polynomial: LDA: #samples * #iterations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_components = 10\n",
    "n_top_words = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1.205s.\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "data, _ = fetch_20newsgroups(shuffle=True, random_state=1,\n",
    "                             remove=('headers', 'footers', 'quotes'),\n",
    "                             return_X_y=True)\n",
    "data_samples = data[:n_samples]\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.233s.\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,\n",
    "                                   max_features=n_features,\n",
    "                                   stop_words='english')\n",
    "t0 = time()\n",
    "tfidf = tfidf_vectorizer.fit_transform(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf features for LDA...\n",
      "done in 0.238s.\n"
     ]
    }
   ],
   "source": [
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english')\n",
    "t0 = time()\n",
    "tf = tf_vectorizer.fit_transform(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.330s.\n"
     ]
    }
   ],
   "source": [
    "# fit NMF model - Frobenius norm\n",
    "t0 = time()\n",
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          alpha=.1, l1_ratio=.5).fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in NMF model (Frobenius norm):\n",
      "Topic #0: just people don think like know good time make way really say ve right want did ll new use years\n",
      "Topic #1: windows use dos using window program os application drivers help software pc running ms screen files version work code mode\n",
      "Topic #2: god jesus bible faith christian christ christians does sin heaven believe lord life mary church atheism love belief human religion\n",
      "Topic #3: thanks know does mail advance hi info interested email anybody looking card help like appreciated information list send video need\n",
      "Topic #4: car cars tires miles 00 new engine insurance price condition oil speed power good 000 brake year models used bought\n",
      "Topic #5: edu soon send com university internet mit ftp mail cc pub article information hope email mac home program blood contact\n",
      "Topic #6: file files problem format win sound ftp pub read save site image help available create copy running memory self version\n",
      "Topic #7: game team games year win play season players nhl runs goal toronto hockey division flyers player defense leafs bad won\n",
      "Topic #8: drive drives hard disk floppy software card mac computer power scsi controller apple 00 mb pc rom sale problem monitor\n",
      "Topic #9: key chip clipper keys encryption government public use secure enforcement phone nsa law communications security clinton used standard legal data\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTopics in NMF model (Frobenius norm):\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.992s.\n"
     ]
    }
   ],
   "source": [
    "# fit NMF model - Kullback-Leibler\n",
    "t0 = time()\n",
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          beta_loss='kullback-leibler', \n",
    "          solver='mu', \n",
    "          max_iter=1000, \n",
    "          alpha=.1,\n",
    "          l1_ratio=.5).fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: people don think just right did like time say really know make said question course let way real things good\n",
      "Topic #1: windows thanks help hi using looking does info software video use dos pc advance anybody mail appreciated card need know\n",
      "Topic #2: god does jesus true book christian bible christians religion faith church believe read life christ says people lord exist say\n",
      "Topic #3: thanks know bike interested car mail new like price edu heard list hear want cars email contact just com mark\n",
      "Topic #4: 10 time year power 12 sale 15 new offer 20 30 00 16 monitor ve 11 14 condition problem 100\n",
      "Topic #5: space government 00 nasa public security states earth phone 1993 research technology university subject information science data internet provide blood\n",
      "Topic #6: edu file com program try problem files soon window remember sun win send library mike article just mit oh code\n",
      "Topic #7: game team year games play world season won case division players win nhl flyers second toronto points cubs ll al\n",
      "Topic #8: drive think hard drives disk mac apple need number software scsi computer don card floppy bus cable actually controller memory\n",
      "Topic #9: just use good like key chip got way don doesn sure clipper better going keys ll want speed encryption thought\n",
      "Fitting LDA models with tf features, n_samples=2000 and n_features=1000...\n"
     ]
    }
   ],
   "source": [
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)\n",
    "\n",
    "print(\"Fitting LDA models with tf features, \"\n",
    "      \"n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: people don think just right did like time say really know make said question course let way real things good\n",
      "Topic #1: windows thanks help hi using looking does info software video use dos pc advance anybody mail appreciated card need know\n",
      "Topic #2: god does jesus true book christian bible christians religion faith church believe read life christ says people lord exist say\n",
      "Topic #3: thanks know bike interested car mail new like price edu heard list hear want cars email contact just com mark\n",
      "Topic #4: 10 time year power 12 sale 15 new offer 20 30 00 16 monitor ve 11 14 condition problem 100\n",
      "Topic #5: space government 00 nasa public security states earth phone 1993 research technology university subject information science data internet provide blood\n",
      "Topic #6: edu file com program try problem files soon window remember sun win send library mike article just mit oh code\n",
      "Topic #7: game team year games play world season won case division players win nhl flyers second toronto points cubs ll al\n",
      "Topic #8: drive think hard drives disk mac apple need number software scsi computer don card floppy bus cable actually controller memory\n",
      "Topic #9: just use good like key chip got way don doesn sure clipper better going keys ll want speed encryption thought\n",
      "Fitting LDA models with tf features, n_samples=2000 and n_features=1000...\n"
     ]
    }
   ],
   "source": [
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)\n",
    "\n",
    "print(\"Fitting LDA models with tf features, \"\n",
    "      \"n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words Limitations\n",
    "- Unigrams can't capture phrases & multiword expressions. It also can't handle misspellings or word derivations.\n",
    "- N-Grams help solve this problem.\n",
    "- Below: corpus of two documents. The 2nd doc contains a misspelling of \"words\".\n",
    "- The vectorizer uses ```char_wb``` to vectorize look at characters inside word boundaries (whitespace)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "[[1 1 1 0 1 1 1 0]\n",
      " [1 1 0 1 1 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "ngram_vectorizer = CountVectorizer(analyzer='char_wb', \n",
    "                                   ngram_range=(2, 2))\n",
    "\n",
    "counts = ngram_vectorizer.fit_transform(['words', 'wprds'])\n",
    "\n",
    "print(ngram_vectorizer.get_feature_names() == (\n",
    "    [' w', 'ds', 'or', 'pr', 'rd', 's ', 'wo', 'wp']))\n",
    "\n",
    "# character-wise 2-gram model finds the document to match\n",
    "# in 4 of 8 documents, which may indicate a better classifier.\n",
    "\n",
    "print(counts.toarray().astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Below: ```char``` analyzer creates n-grams that span across words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# char_wb\n",
    "ngram_vectorizer = CountVectorizer(analyzer='char_wb', \n",
    "                                   ngram_range=(5, 5))\n",
    "\n",
    "ngram_vectorizer.fit_transform(['jumpy fox'])\n",
    "\n",
    "print(ngram_vectorizer.get_feature_names() == (\n",
    "    [' fox ', ' jump', 'jumpy', 'umpy ']))\n",
    "\n",
    "# char\n",
    "ngram_vectorizer = CountVectorizer(analyzer='char', \n",
    "                                   ngram_range=(5, 5))\n",
    "ngram_vectorizer.fit_transform(['jumpy fox'])\n",
    "\n",
    "print(ngram_vectorizer.get_feature_names() == (\n",
    "    ['jumpy', 'mpy f', 'py fo', 'umpy ', 'y fox']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ```char_wb``` is useful in languages that use whitespace for word separation - it generates less noise than ```chr```.\n",
    "- Bag-of-words and Bag-of-ngrams techniques destroy most of the inner structure of documents - most of the meaning is lost. Retrieving this meaning is outside the scope of scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Large text corpus vectorizing -- the hashing trick\n",
    "\n",
    "- Above vectorization techniques require in-memory mapping = not feasible for large corpus datasets.\n",
    "- [Hashing Vectorizer]() helps overcome this problem. It combines [Feature Hashing](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.FeatureHasher.html#sklearn.feature_extraction.FeatureHasher) with [Text Preprocessing & Tokenization (Count Vectorizer)](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.FeatureHasher.html#sklearn.feature_extraction.FeatureHasher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x10 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 16 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "hv = HashingVectorizer(n_features=10)\n",
    "hv.transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Real world: leave ```n_features``` at a default setting of ```2**20```. Reduce it if you see downstream memory or model size issues.\n",
    "- Dimensionality does not affect training time of algorithms that use **CSR** matrices (LinearSVC(dual=True), Perceptron, SGDClassifier, PassiveAggressive), but does for algorithms that use **CSC** matrices (LinearSVC(dual=False), Lasso(), ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defaults - no collisions but much larger output space\n",
    "hv = HashingVectorizer()\n",
    "hv.transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out-of-core scaling\n",
    "- HashingVectorizer can do out-of-core scaling using a mini-batch approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom vectorizer classes\n",
    "- ```preprocessor```: a function that transforms an entire document as a single string. This could be used to strip HTML, etc.\n",
    "- ```tokenizer```: a function that splits the preprocessor's output into a list of tokens.\n",
    "- ```analyzer```: a function that replaces the preprocessor & tokenizer - custom analyzers do this for you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_tokenizer(s):\n",
    "    return s.split()\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer=my_tokenizer)\n",
    "vectorizer.build_analyzer()(u\"Some... punctuation!\") == (\n",
    "    ['some...', 'punctuation!'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips:\n",
    "- If documents are pre-tokenized by an external package, store them in files with tokens separated by whitespace. Pass ```analyzer=str.split```.\n",
    "- Advanced analysis (stemming, lemmatizing, ...) are not included in scikit-learn but can be added using custom tokenizers or analyzers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "class LemmaTokenizer:\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
    "\n",
    "vect = CountVectorizer(tokenizer=LemmaTokenizer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['color', 'color']\n"
     ]
    }
   ],
   "source": [
    "# transform British spelling to American spelling\n",
    "import re\n",
    "def to_british(tokens):\n",
    "    for t in tokens:\n",
    "        t = re.sub(r\"(...)our$\", r\"\\1or\", t)\n",
    "        t = re.sub(r\"([bt])re$\", r\"\\1er\", t)\n",
    "        t = re.sub(r\"([iy])s(e$|ing|ation)\", r\"\\1z\\2\", t)\n",
    "        t = re.sub(r\"ogue$\", \"og\", t)\n",
    "        yield t\n",
    "\n",
    "class CustomVectorizer(CountVectorizer):\n",
    "    def build_tokenizer(self):\n",
    "        tokenize = super().build_tokenizer()\n",
    "        return lambda doc: list(to_british(tokenize(doc)))\n",
    "\n",
    "print(CustomVectorizer().build_analyzer()(u\"color colour\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
